{"meta":{"title":"xixfeng","subtitle":null,"description":"为了什么而生活","author":"Li Yang","url":"https://xixfeng.github.io"},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"about/index.html","permalink":"https://xixfeng.github.io/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... function bot_ui_ini() { var botui = new BotUI(\"hello-mashiro\"); botui.message.add({ delay: 800, content: \"Hi, there111👋\" }).then(function () { botui.message.add({ delay: 1100, content: \"这里是 Mashiro\" }).then(function () { botui.message.add({ delay: 1100, content: \"一个可爱的蓝孩子~\" }).then(function () { botui.action.button({ delay: 1600, action: [{ text: \"然后呢？ 😃\", value: \"sure\" }, { text: \"少废话！ 🙄\", value: \"skip\" }] }).then(function (a) { \"sure\" == a.value && sure(); \"skip\" == a.value && end() }) }) }) }); var sure = function () { botui.message.add({ delay: 600, content: \"😘\" }).then(function () { secondpart() }) }, end = function () { botui.message.add({ delay: 600, content: \"![...](https://view.moezx.cc/images/2018/05/06/a1c4cd0452528b572af37952489372b6.md.jpg)\" }) }, secondpart = function () { botui.message.add({ delay: 1500, content: \"目前就读于上海财经大学\" }).then(function () { botui.message.add({ delay: 1500, content: \"向往技术却误入商科，但后来喜欢上了经济学…\" }).then(function () { botui.message.add({ delay: 1200, content: \"因为数据分析也需要Coder嘛\" }).then(function () { botui.message.add({ delay: 1500, content: \"主攻 R 语言和 Python，略懂 STATA，偶尔也折腾 HTML/CSS/JavaScript/PHP\" }).then(function () { botui.message.add({ delay: 1500, content: \"研究的方向，是经济/金融方向的数据分析（data science）以及机器学习（machine learning）\" }).then(function () { botui.message.add({ delay: 1800, content: \"喜欢画画，希望有一天能够被称为画师\" }).then(function () { botui.action.button({ delay: 1100, action: [{ text: \"为什么叫Mashiro呢？ 🤔\", value: \"why-mashiro\" }] }).then(function (a) { thirdpart() }) }) }) }) }) }) }) }, thirdpart = function () { botui.message.add({ delay: 1E3, content: \"Mashiro以及站名都来自一部动画，因为和主角有一样的爱好~ 如果有兴趣可以找找首页上的视频~\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"为什么是白猫呢？ 🤔\", value: \"why-cat\" }] }).then(function (a) { fourthpart() }) }) }, fourthpart = function () { botui.message.add({ delay: 1E3, content: \"因为对GitHub有种执念… \" }).then(function () { botui.message.add({ delay: 1100, content: \"而且我真的是猫控！\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"域名有什么含意吗？(ง •_•)ง\", value: \"why-domain\" }] }).then(function (a) { fifthpart() }) }) }) }, fifthpart = function () { botui.message.add({ delay: 1E3, content: \"emmmm，看备案信息你就知道了=.= 本来想要zheng.xin的，但50万真买不起。。\" }).then(function () { botui.message.add({ delay: 1600, content: \"那么，仔细看看我的博客吧？ ^_^\" }) }) } } bot_ui_ini()","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://xixfeng.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"client/index.html","permalink":"https://xixfeng.github.io/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"comment/index.html","permalink":"https://xixfeng.github.io/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"donate/index.html","permalink":"https://xixfeng.github.io/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"music/index.html","permalink":"https://xixfeng.github.io/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"rss/index.html","permalink":"https://xixfeng.github.io/rss/index.html","excerpt":"","text":""},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"lab/index.html","permalink":"https://xixfeng.github.io/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"links/index.html","permalink":"https://xixfeng.github.io/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"tags/index.html","permalink":"https://xixfeng.github.io/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://xixfeng.github.io/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"video/index.html","permalink":"https://xixfeng.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"软件工程","slug":"软件工程","date":"2019-06-15T12:01:55.000Z","updated":"2019-06-18T07:21:14.882Z","comments":true,"path":"2019/06/15/软件工程/","link":"","permalink":"https://xixfeng.github.io/2019/06/15/软件工程/","excerpt":"","text":"#软件 软件：指令的集合，通过执行这些指令可以完成预期的功能；数据结构，帮助程序有效利用数据；软件描述信息，描述软件的信息和操作 软件的特点：功能性、可靠性、可用性、可维护性、可移植性、效率 软件于硬件的不同：软件不会”磨损” 软件的改变特性：画图 软件工程 软件工程：在软件的开发、运行和维护中采用系统的、有纪律的、可量化的方法，即将工程应用于软件。 软件神话(myths) 过程模型 软件流程：流程是在创建某些工作产品时执行的活动、操作和任务的集合。 过程模型：过程模型就是一种开发策略，这种策略针对软件工程的各个阶段提供了一套范形，使工程的进展达到预期的目的 过程框架：沟通、策划、建模(需求分析和设计)、构建(实际代码和测试)、部署 过程模型分类：步骤模式、任务模式、阶段模式 惯用过程模型瀑布模型按照过程框架线性开发驱动力：适用于需求清楚、熟悉的系统 v cycle model 增量过程模型在此模型中，强调将项目分为一个个小块，类似并行开发 增量过程模型按照过程框架线性迭代开发驱动力：适用于周期比较短的项目 RAD ModelRAD Model：RAD（Rap Application Development，快速应用开发）模型是软件开发过程中的一个重要模型，由于其模型构图形似字母V，所以又称软件测试的V模型。基于构建开发, 演化过程模型真正的循环迭代 原型开发(Prototyping model)驱动力：瀑布模型的改进，适合需求不清楚的系统原型：一个系统的替代雏形，一般会被抛弃 螺旋模型(spiral model)结合了原型的迭代性质和瀑布模型的可控性和系统性特点 并发模型过程框架中的步骤会同时存在，同时进行驱动力：与螺旋模型相似，经常在client/server程序中使用 专用过程模型 Component based process models object-oriented process models 统一过程统一过程是前面模型的结合，吸取了各个模型的优点各个阶段：起始阶段（inception phase）、细化阶段（Elaboration Phase）、构建阶段（construction Phase）、转换阶段（transition Phase）、生产阶段（production phase） 敏捷开发什么是敏捷？ 快速响应变更 快速与利益相关者交流 敏捷及变更成本驱动力：普遍存在的变更是敏捷的基本动力，敏捷是低成本的持续响应变更 敏捷过程极限编程极限编程（eXtreme Programming，XP）属于敏捷过程，包含策划、设计、编码、测试、发布五个阶段 Scrum 待定项（backlog） 冲刺（sprint） 例会动态系统开发方法（DSDM） 需求工程需求工程（Requirement Engineering，RE）是指致力于不断理解需求的大量任务和技术，是设计和构建之间的桥梁。包含起始 Inception、获取 Elicitation、细化 Elaboration、协商 Negotiation、规格 Specification、确认 Validation、需求管理 Requirements management 用例图 use-cases包含Actor(用户或设备)与Use Case(用例),以及它们之间的关系 用户之间的关系：泛化 用例之间的关系：包含、扩展、泛化 用例文档包含： 用例的唯一标识（UC_ordermeal） 用例名称（点菜） 行为者（小明） 前置条件：完成一个用例需要的条件 后置条件：执行用例之后系统的状态 用例场景（用例的简述）：A、System shows a list of available food boxes to choose fromB、The customer chooses the box(es) he/she likes to have by clicking on the respective buttonC、The customer enters “Confirm” buttonD、The customer chooses 微信or 支付宝 payment buttonE、The customer presses his/her 付款 icon on the smart phone for payment scanning 流程，分主干和分支，描述共前置条件到后置条件的过程中，系统与用户之间有何种交互步骤，这里多见的是时序图或者流程图（小明自己选 or 让服务员推荐，if 服务员推荐，小明接受 or 不接受……确定点某个菜……）； 扩展：一个功能是在另一个功能之上扩展出来的 构建分析模型分析模型：使用不同的元素描述出系统的功能，获取data，function，behavior Requirements validation review, Check List？ 分析模型的元素 基于场景的元素：使用基于场景的方法可以从用户的视角描述系统 基于类的元素：描述出类 行为元素：系统的行为以及发生行为后系统的状态 面向数据流的元素：系统数据的流动 分析模型：基于场景的方法（Scenario-based）基于场景建模创建初始用例、细化用例、编写正式用例 补充用例的UML模型活动图站在用户角度根据步骤描述系统半圆形表示功能、菱形表示分支、圆形表示结束 泳道图描述一个步骤中，参与者和分析类的指责与交互 分析模型：基于类的方法(Class-based)类图 类-职责-协作者类-职责-协作者（Class-Responsibility-Collaborator，CRC）建模 分析模型：基于行为的方法(behavioral)找到行为以及系统的状态 状态图 顺序图 分析模型：面向流程的方法(Flow orienter)设计概念设计模型将分析模型转换为设计模型设计过程：数据设计、架构设计、接口设计、构建级设计 ##良好设计类Complete and sufficient, Primitiveness, High cohesion, Low coupling（实现所有功能， 提供软件完整的试图） 设计概念 抽象 体系结构 模式 关注点分离 模块化 信息隐蔽 功能独立 求精 方面 重构：就是通过调整程序代码改善软件的质量、性能，使其程序的设计模式和架构更趋合理，提高软件的扩展性和维护性。 面向对象的设计概念 设计类：原始性、高内聚、低耦合 依赖倒置 质量属性 功能性 易用性 可靠性 性能 体系结构设计体系结构：是指系统的一个或者多个结构，它包括软件构件、构件的外部可见属性以及他们之间的相互联系 体系结构风格 以数据为中心的体系结构 数据流体系结构 调用和返回体系结构 面向对象体系结构 层次体系结构 体系结构考虑要素 经济性 易见性 隔离性 对称性 应急性 构件级设计构件：是计算机软件中的一个模块化的构造块，系统中模块化的、可部署的、可替换的部件构件级设计包括：类，属性，操作，接口 什么是组件？站在面向对象(OO)的角度：一个构建包含一组相互协作的类站在常规(Conventional)的角度：由数据结构、处理逻辑、接口组成；实现处理逻辑的内部数据结构，以及允许调用组件并将数据传递给组件的接口 设计基于类的构件The steps of OO Component-level Design？ 基本设计原则 开闭原则（OCP）：对扩展具有开放性，对修改具有封闭性 Liskov替换原则（LSP）：子类可以替换他们的基类 依赖倒置原则（DIP）：依赖与抽象而不依赖与具体 接口分离原则（ISP）：多个客户转哟个接口比一个通用接口要好 发布复用等价性原则（REP） 共同封装原则（CCP） 共同复用原则（CRP） 构件级设计指导方针接口、依赖与继承 内聚性耦合性用户界面设计黄金原则 把控制权交还给用户 减轻用户记忆负担 保持界面一致 User Interface analysis and designuser analysis, task and work environment analysis，Interface design, Interface validation Interface analysisSteps of interface analysis Interface design StepsDesign GUI according to Use-Case Diagram 测试测试的顺序 unit testing integrationg testing &amp; regression testing Acceptance testing（ Validation testing ） system testing 可测性（testability）的特征operability、observerability、controllability、decomposability、simplicity、stability、understandablity unit testingunit testing需测的单元 interface local data structure boundary conditions independent paths error handing paths integration testing top-down integration：no drivers，has stubs button-up integration：no stubs regression test：为什么集成测试是回归测试的重要部分？因为在集成测试情况下，每加入一个新的模块都有可能会导致原有的模块错误，而回归测试只是重新执行原有测试的子集，以确保变更没有传播不期望的副作用 smoking test：“冒烟测试”这一术语描述的是在将代码更改嵌入到产品的源树中之前对这些更改进行验证的过程。time-critical(时间关键性),rolling(滚动) system testingUse-Case Diagram?Function testing, specification Smoke test冒烟测试就是当我们修改一部分代码后，对修改代码进行测试以及程序主干进行测试 白盒测试 白盒测试：是按照程序内部的结构测试程序，通过测试来检测产品内部动作是否按照设计规格说明书的规定正常进行，检测程序中的每条通路是否都能按预定要求正确工作 Basis path testing 流图(Flow Graph Notation) 环路复杂度(Cyclomatic Complexity): IF语句的个数+1，假如存在if(a&gt;10 &amp;&amp; a&lt;20)这个算两个IF语句，在流图中需要拆开 while中的判断也算？ control structure testing黑盒测试 黑盒测试：根据被测试程序的功能来进行测试 等价类划分(Equivalence Partitioning)：等价划分是一种黑盒测试方法，它将程序的输入域划分为可以从中派生测试用例的数据类。 边界值分析(Boundary Value Analysis) 二者的区别黑盒测试（功能测试）是从用户的观点，按规格说明说要求的输入数据与输出数据对应关系设计测试用例（结构设计）是根据外部特征进行测试白盒测试是根据程序内部逻辑结构进行测试 复习pptFLow oriented? . 4 P’sPeopleProductProcessProject. SCM：The concept of SCM：软件配置管理(SCM)是指通过执行版本控制、变更控制的规程，以及使用合适的配置管理软件，来保证所有配置项的完整性和可跟踪性 SCM process：▪ 配置项识别▪ 工作空间管理▪ 版本控制▪ 变更控制▪ 状态报告 . Risk management:Project, Technology, Business risk . 关注点分离：任何复杂的问题如果被细分成可以独立解决的部分，就可以更容易地处理。 . List the types of models that might be used in requirements modeling and explain the role of each type of model. Scenario-based (system from the user’s point of view) Class-oriented (defines objects, attributes, and relationships) Behavioral (show the impact of events on the system states) Flow-oriented (shows how data are transformed inside the system) or Data . What is umbrella Activities(普适性活动)? List at least five of umbrella activities 普适性活动就是在软件工程中常用的、通用的事物.项目追踪和控制、风险管理、软件质量保证、技术评审(reviews)、软件环境管理 .结合所学的软件工程知识去解决实际问题，比如需求不明确时使用瀑布模型的缺点，需求工程的重要性 ．List the four design models required for a complete specification of a software design and the role of each. Data design - high level model depicting user’s view of the data or information. Architecture design – shows relationships and collaborations among specific analysis model software and hardware elements Interface design - interface depicts a set of operations that describe the externally observable behavior of a class and provides access to its operations Component-level design - Describes the internal detail of each software component . What is UML ? Which UML diagrams are useful for analysis modeling(at least 4 UML diagrams)? Give a short description for each one. 1. A use-case diagram is created to visualize the interaction of your system with the outside world. 2. An activity diagram shows the flow of events within our system. 3.A Class diagram shows an overview of the target system by describing the objects and classes inside the system and the relationships between them. 4.A State diagram shows the behavior of a single object, specifying the sequence of events that an object goes through during its lifetime in response to events. 5. A sequence diagram shows step by step what must happen to accomplish a piece of functionality provided by the system. 6. A collaboration diagram(泳道图) displays object interactions organized around objects and their links to one another. . What are the fundamental differences between the Structured Analysis(SA) and Object-Oriented Analysis(OOA) strategies for requirements analysis? 结构化分析：一种面向数据流的传统软件开发方法，以数据流(过程)为中心构建软件的分析模型、设计模型和实现模型 面向对象分析：它以类为中心构建分析模型，着重于类的定义以及它们相互协作以影响客户需求的方式","categories":[{"name":"技术","slug":"技术","permalink":"https://xixfeng.github.io/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xixfeng.github.io/categories/技术/"}]},{"title":"操作系统","slug":"操作系统","date":"2019-06-13T07:33:33.000Z","updated":"2019-06-18T04:20:38.568Z","comments":true,"path":"2019/06/13/操作系统/","link":"","permalink":"https://xixfeng.github.io/2019/06/13/操作系统/","excerpt":"","text":"计算机组成 处理器(CPU)：控制计算机的操作，执行数据处理功能 内存(memory)：存储数据和程序 输入/输出模块(I/O modules)：在计算机与外部环境之间移动数据 系统总线：在处理器、内存和输入/输出模块间提供通信的设施 操作系统概述操作系统:控制应用程序执行的程序，是应用程序与计算机硬件之间的接口，负责管理计算机硬件和软件资源目标： 方便 有效 扩展能力主要成就: 进程 调度和资源管理 内存管理 信息保护和安全 多个处理系统简单批处理系统简单批处理系统的中心思想是监控程序。将命令一次性输入，交由监控程序执行。 作业控制语言(Job Control language,JCL)是简单批处理系统的处理语言 用户模式，内核模式只有后者能执行特权指令 多道批处理系统解决处理器空闲状态(等待I/O)，内存空间可以保存常驻监控程序和两个用户程序，那么当一个作业需要等待I/O时，处理器可以切换到另一个程序执行。这种处理成为多道程序处理(multiprogramming)或多任务处理(multitasking) 分时系统多道程序设计允许处理器同时处理多个批作业，还可以处理多个交互系统。对于后者而言，由于多个用户分享处理器时间，因而该技术称为分时系统(time sharing) 总结：简单批处理系统内存中只能容纳一个用户程序和操作系统(常驻监控程序)，无法在等待时切换，而多道可以容纳多个用户程序，又因为多道允许处理多个批处理（batch processing)而形成了分时系统 分时系统和多道批处理系统的差别：多道批处理系统是为了提高cpu利用率，分时系统是为了减少响应时间 现代操作系统的特征单体内核(monolithic kernel)：操作系统的大部分功能都由大内核来提供 微内核(microkernel architecture)：只给内核分配一些最基本的功能，其他服务则由运行在用户模式的其他线程提供 多线程(multithreading)：把执行一个应用程序的进程划分为可以同时运行的多个线程 进程进程(进程映像，是进程执行的上下文环境)由程序代码+进程控制块（process control block，PCB)+数据 PCB:由标识符、状态、优先级、程序计数器、内存指针、上下文数据、I/O状态信息、记账信息组成 五状态模型 新建态：刚刚创建的进程，操作系统还未把它加入可执行进程组，它通常是进程控制块已经创建但还未加载到内存中的新进程 五状态模型的缺点：可能全部进程都进入了阻塞态（因为I/O），而此时cpu就会空闲 ##七状态模型当内存中不存在就绪态的程序时，操作系统就把阻塞的进程换出到磁盘的挂起队列中(suspend queue)，而此时就必须要增加另一个状态：挂起态而挂起态又可分为阻塞/挂起态和就绪/挂起态 执行模式 非特权模式：用户模式 特权模式：系统模式、内核模式、控制模式 使用两种模式的原因是保护操作系统和重要的操作系统表(如进程控制块)不受用户程序的干扰。在内核模式下，软件会完全控制处理器及其所有指令、寄存器和内存。为安全起见，这种级别的控制对用户程序来说完全没有必要。 线程线程和进程的区别： 线程：可以分配的工作单元，轻量级进程(LWP) 进程：资源所有权的单元 用户级线程和内核级线程都是指在一个进程中的线程管理 用户级线程用户级线程(user-level thread,ULT)中管理线程的所有工作都由应用程序完成优点： 所有线程数据结构都在一个进程的用户地址空间中，线程切换不需要内核模式特权，节省了两次状态转换的开销 调度因应用程序的不同而不同 ULT可在任何操作系统中运行，不需要对底层内核进行修改以支持ULT 缺点： 无法将一个进程中的多个线程调度到多个处理器 在典型的操作系统中，许多系统调用都会引起阻塞。因此，在ULT执行一个系统调用时，不仅会阻塞这个线程，也会阻塞进程中的所有线程 内核集线程内核集线程(kernel-level thread,KLT)中管理线程的工作均由内核完成优点： 可以同时把一个进程中的多个线程调度到多个处理器 当前线程阻塞时，系统可以调用其他进程中其他线程 内核自身也是可多线程的 缺点： 切换模式十分消耗时间 并发性：互斥和同步 原子操作：一个函数或动作由多个指令的序列实现，要保证这个序列要么都执行，要么都不执行 临界区(critical section)：一段代码，在这段代码中进程将访问共享资源，当另外一个进程已在这段代码运行时，其他进程就不能在这段代码中运行 死锁：两个或两个以上的进程都在等待其他进程昨晚某些事情而不能继续执行的情况 活锁：两个或两个以上的进程为响应其他进程中的变化而持续改变自己的状态但不做有用功但清形 互斥(mutual exclusion)：当一个进程在临界区访问共享资源时，其他进程不能进入该区域 竞争条件(race condition：多个线程或进程在读写一个共享数据时，结果依赖于他们执行的相对时间的情形 饥饿：一个可运行的进程尽管能继续执行，但被调度程序无限期地忽视，而不能执行但情况 临界资源(critical resource)互斥：硬件的支持 禁用中断、启用中断 专用及其指令 信号量 信号量：用于进程间传递信号的一个整数值 二元信号量：只取0值或1值的信号量 互斥量：类似于二元信号量。关键区别在于为其加锁(设定值为0)的进程和为其解锁(设定值为1)的进程必须为同一个 二元信号量来自信号量但是二者的semWait和semSignal有点不同 信号量struct semaphore{ int count; queueType queue; }; void semWait(semaphore s){ s.count--; if(s.count&lt;0){ //把当前进程加入队列 //阻塞当前进程 } void semSignal(semaphore s){ s.count++ if(s.count &lt;= 0){ //将进程P从队列中移除 //把进程P插入就绪队列 } } } 互斥量struct binary_semaphore{ enum {zero, one} value; queueType queue; }; void semWaitB(binary_semaphore s){ if(s.value == one) s.value = zero else{ //将当前进程插入队列 //阻塞当前进程 } void semSignalB(binary_semaphore s){ if(s.queue is empty()) s.value = one; else{ //把进程p从等待队列中移除 //把进程p插入就绪队列 } } } 并发：死锁和饥饿 可重用资源：可重用资源是指一次仅供一个进程安全使用且不因使用而耗尽的资源 可消耗资源：是指可被创建和销毁的资源 死锁的条件 互斥 占有且等待 不可抢占 循环等待：存在一个封闭的进程链，每个进程至少占有次链中下一个进程所需的一个资源 资源分配图 死锁预防使上述四个条件之一无法满足，但互斥无法禁止缺点：会导致低效的资源利用 死锁避免 若有一个进程的请求会导致死锁，则不启动该进程 若有一个进程增加的资源请求会导致死锁，则不允许这一资源分配 银行家算法银行家算法：在分配给线程资源之前，能不能保证安全状态安全状态：指至少有一个资源分配序列不会导致死锁 缺点： 必须事先指导每个进程请求的最大资源数 所讨论的进程必须是无关的，即他们的执行顺序没有任何同步要求的限制 分配的资源数量必须是固定的 在占有资源的时，进程不能退出 死锁检测死锁检测算法检测目前这种资源情况是否会引起死锁，如果引起死锁是因为哪些线程 恢复既然检测出死锁那么如何避免死锁 取消所有死锁进程 将死锁进程回滚 连续取消死锁进程直到不再存在死锁 连续抢占资源 哲学家qia饭问题内存管理伙伴系统(Buddy system)：一种综合内存固定分区与动态分区策略 内存管理的需求 重定位：把进程重定位(relocate)到内存到不同区域 内存分区 逻辑地址：是指与当前数据在内存中的物理地址无关的访问地址 分页页(page) 页(page)：进程中的块 页框(frame)：内存中的块 页表(page table)：给出页对应的页框 固定分区(fixed partitioning)：将系统内存分为大小固定的块,但是这些块大小可以相等，也可以不等(就是实现就把内存给划分好了) 分段段(segment) 根据段号找到物理地址，再将物理地址与偏移量相加 虚拟内存 虚拟内存：在存储分配机制中，尽管备用内存是主存的一部分，但它也可被寻址 虚拟地址：在虚拟内存中分配给某一位置的地址 常驻集(resident set)：常驻集指虚拟页式管理中给进程分配的物理页面数目(内存)。 分页TLB 置换策略在使用虚拟分页的时候，我们会出现一些页在内存中，一些页在辅存中，当我们需要从辅存中取出页时，应该换出内存中哪一页，这就是置换策略 OPT：替换未来中最后使用的页 LRU：置换内存中最长时间未被引用的页 Clock(使用位)：使用位初始为1，某页被访问后使用位置为1，指针在扫描到0时停止，当指针扫过页时，变为0。 Clock(使用位,修改位)：老大就是既被使用也被修改的块 1.寻找(0,0)的页 2.若1失败，查找(0,1),将每个扫描到的使用位变为0 3.第二步失败，重复1和2 调度算法 长程调度：新建态-&gt;就绪/挂起态 新建态-&gt;就绪态 中程调度：就绪/挂起态-&gt;就绪态 短程调度：就绪-&gt;运行态 调度算法研究的是短程调度 服务时间： 进程占用cpu的时间 周转时间：进程等待时间+服务时间 归一化周转时间：周转时间/服务时间 分配器(dispatcher)：在短程调度的时候，将cpu操作权限交付给进程 选择函数(selection function) FCFS 轮转(RR)：每个线程占用cpu一段时间,VRR避免了处理器密集型比I/O密集型占用了更多的cpu时间 最短进程优先(SPN,shortest Process Next)：非抢占 选择预计处理时间最短的 最短剩余时间(SPT)：在最短进程优先上加入抢占机制 最高响应比优先(HRRN)：最小归一化周转时间，当前进程完成或堵塞时，选择R值最大当就绪进程。不可抢占 反馈法 决策模式(decision mode)：抢占式还是非抢占式 HRRN的优点：FCFS方式只考虑每个作业的等待时间而未考虑执行时间的长短，而SPT方式只考虑执行时间而未考虑等待时间的长短。HRRN既考虑了等待时间也考虑了执行时间，避免了线程的饥饿 输入、输出文件 三种I/O技术程序控制的I/O、中断控制的I/O、直接存储器访问(DMA) 寻道时间：将磁头臂移动到指定磁道道时间 旋转延迟：将磁盘待访问的区域移动到读/写磁头可以访问到的位置所需的时间 传输时间：向磁盘读取/输入的时间，磁盘的旋转速度为3600-15000rpm，其中15000rpm相当于每4ms旋转一周 平均时间：从头到中所需的时间磁盘调度策略 FIFO SSTF：找到最近的磁道，可能使得某些请求一直处于饥饿状态 SCAN：电梯算法，向固定方向移动，完成沿途的读取，到达末端后弹回 C-SCAN：到达末端后弹回首端磁道 文件管理 域(field)：是最基本的文件结构 记录(record)：一组域的集合，可视为应用程序的一个单元 文件(file)：一组记录的集合 数据库(database)：一组相关数据的集合 文件组织和访问文件系统：文件系统是操作系统用于存储设备或分区上的文件的方法和数据结构选择文件组织的原则：1.快速访问 2.易于修改 3.节约存储空间 4.简单维护(maintenance) 5.可靠性(reliability) 层次结构 文件组织(file organization)是指文件中记录的逻辑结构 堆：堆(pile)是最简单的文件组织结构，因而对记录的访问是通过穷举查找方式进行的 顺序文件：关键域+顺序存储 索引顺序文件：关键域+顺序存储+索引，这种方式能找到离目标文件最近的位置 索引文件：每个域都能成为索引对象 直接文件或散列文件： 辅存管理文件分配表(FAT,File Allocation Table)：为跟踪分配给文件的分区，应该使用那种数据结构，也决定了分配给文件何种空间 连续分配(contiguous allocation)：在创建文件时，分配一组连续的空间给文件 链式分配(chained allocation)：链式分配基于单个块，每个块的末尾都包含指向下一块的指针 索引分配(index allocation)：将文件名与索引块放入文件分配表中，再在索引块中存储该文件的所有块","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://xixfeng.github.io/tags/操作系统/"}],"keywords":[]},{"title":"线性数据结构与非线性数据结构","slug":"线性数据结构与非线性数据结构","date":"2018-12-27T10:30:18.000Z","updated":"2019-06-12T08:10:30.140Z","comments":true,"path":"2018/12/27/线性数据结构与非线性数据结构/","link":"","permalink":"https://xixfeng.github.io/2018/12/27/线性数据结构与非线性数据结构/","excerpt":"","text":"线性数据结构线性数据结构往往适用于小型的数据存储，因为它的增删查改基本上都是n，当数据量过大时，时间复杂度太高 但是线性数据结构往往是用于计算的优秀工具 非线性数据结构树数用于存储大型文件，比线性结构更具有优势","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://xixfeng.github.io/tags/数据结构/"}],"keywords":[]},{"title":"机器学习","slug":"机器学习","date":"2018-12-02T07:49:28.000Z","updated":"2019-06-12T08:10:53.959Z","comments":true,"path":"2018/12/02/机器学习/","link":"","permalink":"https://xixfeng.github.io/2018/12/02/机器学习/","excerpt":"","text":"机器学习定义definete：A computer program is said to learn from experience E with respect to some task T and some performance measureP, if its performance onT, as measured by P, improves with experienceE 例如在象棋机器学习中，E就是下象棋的过程，T就是获得胜利，P就是下一次与人下棋时，胜利的概率 邮件分类中，E就是我们把某一个邮件归结为辣鸡，T就是把邮件分类，P就是下一次遇到新邮件时，成功归类的概率 学习算法监督学习、无监督学习：有没有一个明确的值，比如新闻分类问题就没有的值，而判断是否有癌症就有明确的值 分类问题：给特征值，分类，离散的 回归问题：给特征值，预测数值，连续的 偏斜类：结果的不同情况概率本身差距很大 precision查准率 recall回调率 解决偏斜类问题 查准率：预测正确成功的概率 回调率：预测错误成功的概率 评估查准率和回调率 如何优化算法 1.选择合适的模型 将数据6 2 2划分，一部分作为训练数据，一部分作为（交叉）验证数据 ，一部分作为测试数据 使用交叉验证来得到最合适的模型，测试数据来得到他的错误率 2.解决过拟合，欠拟合问题 3.学习曲线 解决是否增加数据集的问题 判断两条曲线之间距离是否过大 4.神经网络 5.PCA算法 将数据降维，更快的进行计算 降维矩阵？可以根据这个矩阵降维，还原 KNN算法 根据特征值建立维度，通过未知物品在这个维度中与其他物品的“距离”来分类 先分类，然后按照更群的距离对该对象进行归类 步骤 处理数据 数据向量化（处理到一个特征维度） 计算欧几里得距离（核心） 根据距离进行分类 from numpy import * import operator from os import listdir #这个数据集的行列代表什么，数值代表什么？ #这个数据集的每一行代表一个“物品”（不同两行可能代表相同“物品”） 每一列代表一个属性 其中的数值代表特征值 #(个数，测试数据集，训练数据集，分类名称) def knn(k,testdata,traindata,labels): #得到行数 tile是numpy里面的 traindatasize = traindata.shape[0] #(需扩展的数组，（行扩展倍数，列扩展倍数）) dif = tile(testdata,(traindata.size,1))-traindata sqdif = dif**2 #将每一行相加 sumsqdif = sqdif.sum(axis=1) dis = sumsqdif**0.5 #排序 得到下标序号 sortdis = dis.argsort() count = {} for i in range(0,k): vote = labels[sortdis[i]] count[vote] = count.get(vote,0)+1 sortcount = sorted(count.items(),key=operator.itemgetter(1),reverse=True) return sortcount[0][0] #读取文件变为一维数组 def datatoarray(fname): arr = [] fh = open(fname) for i in range(0,32): thisline = fh.readline() for j in range(0,32): arr.append(int(thisline[j])) return arr #取文件前缀，得到lables def seplabel(fname): filestr = fname.split(&quot;.&quot;)[0] label = int(filestr.split(&quot;_&quot;)[0]) return label #得到训练数组和标签数组 def traindata(): labels=[] trainfile = listdir(&quot;D:\\项目\\数据挖掘\\数据\\图片&quot;) num = len(trainfile) #长度1024（列），每一行存储一个文件 #用一个数组存储所有训练数据，行：文件总数，列：1024（32*32存储每一个像素点） trainarr = zeros((num,1024)) for i in range(0,num): thisfname = trainfile[i] thislabel = seplabel(thisfname) labels.append(thislabel) trainarr[i,:] = datatoarray(&quot;traindata/&quot;+thisfname)#赋予二维数组一行数据 return trainarr,labels 回归梯度下降算法 同步更新 其中a为学习率，偏导积分项为该变量的“斜率”。 纠结点 开始有点纠结这里的a代表横轴还是竖轴，但根本不用纠结。因为这里的a就只是一个控制下降快慢的参数不存在什么横竖 为什么会出现导数？因为求偏导后的正负能帮我们移动参数到底端 导数的大小并不影响 当a过小时，会导致算法速度太慢；当a过大时，会导致参数无法收敛，甚至发散 参数可能会局部收敛 这个导数求出来是函数变化最快的地方,即往这个方向走，函数值更容易下降？（但是我觉得更容易下降并没有什么意义） 一些技巧 特征收缩（feature Scaling） 特征收缩使得我们更快的到达收敛 那么为什么特征量过大会导致我们算法时间变长呢？因为当特征值过大时，这个特征值需要改变很多才能改变因变量，导致函数拉长，收缩的“路径”变长 每一项除以最大值，使范围为 -1到 1之间 保证处理过后的特征值的平均值在0左右 2.关于学习率 a的取值 a太大容易出问题，a太小速度太慢 3.减少特征值 代价函数根据代价函数来计算我们的假设函数与真正值之间的偏差 拟合问题过拟合、欠拟合 解决方法 减少特征量的数目 手动选择，模型选择算法 正则化 不改变特征量数目，但是降低量级或者降低theta的值 当我们有很多特征值，并且每个特征值对结果都有影响 正则化通过给想要降低的theta值添加“惩罚函数”，来降低影响较小的theta的值，使假设函数更加简单 为什么不用对theta0降值呢？ 问题，如何知道哪个参数对结果影响小呢？并不能知道，所以我们会给所有的参数都加上惩罚函数 lambda两个目的 使假设更好的拟合数据集 使假设函数相对简单，避免出现过拟合 lambda参数需要平衡这两个目标 当lambad参数过大时，会使函数欠拟合，当过小时，会发生过度拟合的情况 线性回归y = a+bx 次数不大于1 方法一 梯度下降算法代价函数：通过求代价函数的最小值，帮我们得到最准确的参数的值。我们一般选择平方差作为代价函数。 ​ 利用等高线来表示多维函数 特征值大于等于1时 加入正则化改变代价函数，更新梯度下降中的J 加入正则化后，梯度下降的速率并没有改变 方法二 正规方程直接用正规方程，算出令代价函数最小的参数的值 ​ X matrix ​ 直接计算 加入正则化记住，千万不要正则化X0，因为这个X0是我们人工加进去的，theta的取值和它无关， 这里有一点很难受的地方，octave下标都是从1开始计算的 多项式回归当特征值（包括人工组合featureMap的特征值）越多，参数越准确，但是可能会出现过拟合情况 代价函数没变，梯度下降算法没变 逻辑回归逻辑回归和线性回归的区别就是 可以得到概率 没有选择均方差作为代价函数 代价函数和假设函数间接相关 功能：判断是非问题，例如判断邮件是否为垃圾邮件 假设函数 选择合适的多项式 代价函数，和线性回归代价函数不一样 加入正则化改变代价 神经网络我们的神经网络，每一个神经节点传给下一层的都是0、1值 theta n = L(n+1) * Ln+1 a n = L(n) * Ln+1 z (n+1) = an theta n’ = L(n) L(n+1) an(n+1) = sigmoid(z(n+1)) 不要所有的theta值都设置为0， 神经网络 第j层第i个神经元的激励函数 代价函数，其中的 htheta代表最后一层的theta,k代表第几个输出3，我觉得这个代价函数有问题，因为没用运用隐藏层的theta（我理解有问题，注意看题目中的h代表什么），所以用一般的求偏导的算法很难算出梯度 注意我们需要每一层额外添加一个无影响的参数，详情参考作业1，因为我们的逻辑回归有一个没有x项的theta0 且是计算玩过后添加，因为计算前我们相对于上一层是不需要添加的 反向传播算法反向传播算法就是神经网络的一个逆向计算，计算的D就是参数梯度。 不要将所有theta设置为零或者相同的数，这会使得我们隐藏层“学习”不到东西，这与反向传播算法计算的梯度有关 梯度检验梯度检验计算量十分大，没用必要不用开启 Octave算法 分类问题参考作业ex3，在这个作业中，并没有使用神经网络，但是训练一个分类器，重点是利用逻辑回归判断，这个东西是本类还是不是本类 朴素贝叶斯贝叶斯分类： import numpy as npy import operator class Bayes: def __init__(self): self.length = -1 self.labelcount = dict() self.vectorcount = dict() #(已知物体，这些物体的类别) def fit(self,dataSet:list,labels:list): if(len(dataSet)!=len(labels)): return ValueError(&quot;输入的数据数和他们所属的类别数不一样&quot;) self.length = len(dataSet[0])#特征数数量 labelnum = len(labels) norlabels = set(labels) for item in norlabels: self.labelcount[item] = labels.count(item)/labelnum#计算每个类别的概率 #实现同时遍历两个列表 #将每个物体的特征数组存入对应的字典中 for vetor,label in zip(dataSet,labels): if(label not in self.vectorcount): self.vectorcount[label] = [] self.vectorcount[label].append(vetor) #计算出每个特征量在不同类中的概率 #1.行相加2.每一个特征值除以总行数 （记住每个特征值相互独立） for item in self.vectorcount: tem = npy.array(self.vectorcount[item]) rowsum = tem.shape[0]#行数 temcount = tem.sum(axis=0)#0不同行相加 1不同列相加 self.vectorcount[item] = temcount/rowsum print(&quot;训练完毕&quot;) return self def btest(self,testdata): if(self.length == -1): return ValueError(&quot;你还没有训练&quot;) tem = npy.array([]) pdict = {} for label in self.vectorcount: temp = 1 thisdata = self.vectorcount[label] thisp = self.labelcount[label] for item in range(len(testdata)): if testdata[item] == 1:temp = temp*thisdata[item] else: temp = temp*(1-thisdata[item]) temp = temp * thisp pdict[label] = temp sortcount = sorted(pdict.items(),key=operator.itemgetter(1),reverse=True) print(sortcount[0]) # for item in self.vectorcount: bayes = Bayes() dataSet = [[1,0],[1,0],[0,1],[0,1],[1,1]] labels = [&quot;学习好&quot;,&quot;学习好&quot;,&quot;运动好&quot;,&quot;运动好&quot;,&quot;学习好&quot;] bayes.fit(dataSet,labels) bayes.btest([1,0]) 数学知识导数求偏导相当于一元函数找变化率，（把这个图像映射为一元函数，因变量为f，自变量为x） 矩阵单位矩阵 I A = A I = A矩阵的逆 线性代数标准差 标准差是方差的算术平方根 octave：这个符号有点东西 用来表示范围省略 1:3 代表取1到3 : 代表从开头取到结尾 [1 3]代表取1和3 . 一般代表元素的运算 octave的 p = zeros(1,m)和 p1 = zeros(m,1)是不一样的，如果p = p1有时候会出现问题 计算 sqrt(10) 开平方 exp(10) 以e为底，10为指数的幂运算 abs(10) 取绝对值 floor(a) 向下取整 ceil(a) 向上取整 max(A,B) 返回一个由A B中最大元素组成的矩阵 max(A,[],1) 1返回每一列最大值 2返回每一行的最大值 max(A) 默认求每一列最大值 sum(A,1) 计算每一列相加 sum(A,2) 计算每一行相加 矩阵相关 disp(a) disp(sprintf(&#39;2 decimal: %0.2f&#39;,a)) A = [1 2;3 4;5 6] ;分割行 ones(2,3) 生成2行三列全部为1的矩阵 zeros(1,2) rand(1,3) 生成1行3列的随机矩阵 randn(1,3) 随机生成满足高斯的矩阵 平均值为0 标准差为1 eye(3) 生成单位矩阵 help eye size (a) 返回a矩阵的size A(:,2)返回第二列所有数据 A = [A,[1;2;3]] 给A增加一列 A(:) 把A变成一个列向量 A = [A B]组合A B两个矩阵 A = [A;B] A &#39; 求A的转置 find(A&lt;3) 返回索引 A&lt;3 返回对应的bealoon值 系统操作 load name 加载文件 who 显示所有变量 clear x 删除变量 save hello.dat x 将x变量保存在本地 保存为二进制 save hello.txt v -ascll 画图 plot(x,y) hold() title(&quot;my plot&quot;) print(&quot;myPlot.png&quot;) close subplot(2,2,1) 分图 axis(p[0.5 1 1 2]) 限制坐标范围 imagesc(A),colorbar,colormap gray 绘制一个有颜色的图 控制语句 while i &lt;=5, i = i+1; end; for i = 1:10, disp(i); end; if i == 1, disp(i); end; 函数 function y = squareThisNumber(x) y = x^2; function [y1,y2] = squareThisNumber(x) y1 = x^2; y2 = x^3; addpath() 帮助找到我们自定义的function函数 fminunc 这个函数会自动帮你调整学习 这个函数会自动帮你调整学习率，只需要弄好梯度和代价函数tions = optimset(‘GradObj’, ‘on’, ‘MaxIter’, 400)[theta,cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);%这个是必须要加的，之前也没有定义的参数%这个函数最开始会把initial_theta赋值给t，然后调用costfunction函数%第一次调用完过后会使用我们的costFunction结果中的grad，将theta = theta - a*grad%再把theta赋值给t，进行二轮计算%！！！！！所以在fminunc里面需要返回Jvalue和grad（梯度）","categories":[],"tags":[],"keywords":[]},{"title":"数据挖掘","slug":"数据挖掘","date":"2018-11-28T04:00:38.000Z","updated":"2019-06-12T08:10:59.142Z","comments":true,"path":"2018/11/28/数据挖掘/","link":"","permalink":"https://xixfeng.github.io/2018/11/28/数据挖掘/","excerpt":"","text":"数据挖掘 机器学习 大数据之间的关系大数据是媒体炒热的一个概念：总的来说就是数据量大，结构复杂，数据更新快 机器学习方法在大型数据库中的应用称为数据挖掘(Data Mining) 数据挖掘数据挖掘：从大量的数据中获取知识，模式 数据规约 主成分分析 ​ 文本挖掘jieba词性表 import jieba import jieba.posseg import jieba.analyse str = &quot;我爱东方上海明珠&quot; #全模式 tem = jieba.cut(str,cut_all=True) #精准模式 tem = jieba.cut(str,cut_all=False) #搜索引擎模式 tem = jieba.cut_for_search(str) #默认使用精准引擎模式 tem = jieba.cut(str) #查看词性 tem = jieba.posseg.cut(str) #加载用户自己的词典 #jieba.load_userdict() #提取用户词频最高的几个单词 tag = jieba.analyse.extract_tags(str,3) #标识出句子中词的位置 tem = jieba.tokenize(str) #分析遮天 data = open(&quot;zhetian.txt&quot;,encoding=&quot;UTF-8&quot;).read() tem = jieba.analyse.extract_tags(data,20) print(tem) 文本相似度分析TF-IDF算法：是一种用于数据检索和数据挖掘的常用加权技术，用来确定文件（文字）在文件集（文本）中的重要性。 权值大小和文字在文本中出现的频率成正比，和在词典中出现的频率成反比 步骤 读取文档 对要计算的多篇文档进行分词 对文档（分词后）进行整理成指定格式 计算出赐词语的频率 【可选】对频率低的词进行过滤 通过语料库建立词典 加载要对比的文档 将要对比的文档通过doc2bow转化为稀疏向量 对稀疏向量进行进一步处理，得到新语料库 将新语料库通过tfidfmodel处理，得到tfidf 通过token2id得到特指数 洗漱矩阵相似度，从而建立索引 得到最终结果 from gensim import corpora,models,similarities import jieba from collections import defaultdict doc1 = &quot;zhetian.txt&quot; doc2 = &quot;shengxu.txt&quot; def gettext(): data = open(doc1,encoding=&quot;utf-8&quot;).read() data1 = open(doc2,encoding=&quot;utf-8&quot;).read() return data,data1 def textcut(data,data1): tem = jieba.cut(data) tem1 = jieba.cut(data1) return tem,tem1 def textarrange(word,wrod1): str = &quot;&quot; str1 = &quot;&quot; for item in word: str += item + &quot; &quot; for item in word1: str1 += item + &quot; &quot; return str,str1 data,data1 = gettext() word,word1 = textcut(data,data1) str,str1 = textarrange(word,word1) #将文本的词语合并 documents = [str,str1] #创建二维数组，外层为行 #split筛选去掉了标点符号 texts = [[word for word in document.split()] for document in documents] #这个词典的key可以随意，但value必须事先定义 frequency = defaultdict(int) for text in texts: for token in text: frequency[token] += 1 #保存 dictionary = corpora.Dictionary(texts) dictionary.save(&quot;wenben.txt&quot;) testdoc = open(&quot;test.txt&quot;,encoding=&quot;utf-8&quot;).read() testdata = jieba.cut(testdoc) tem = &quot;&quot; for item in testdata: tem += item + &quot; &quot; new_vec = dictionary.doc2bow(tem.split()) corpus = [dictionary.doc2bow(text) for text in texts] tfidf = models.TfidfModel(corpus) featureNum = len(dictionary.token2id.keys()) index = similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featureNum) sim = index[tfidf[new_vec]] print(sim) 数据建模数据建模是指对现实世界各类的抽象组织，建立一个合适的模型对数据进行处理。在数据分析和挖掘中，我们通常需要根据一些数据建立起特定的模型。模型的建立需要依赖于算法，常见的算法分类、聚类、关联、回归 python数据分类的实现过程 明确需求并对数据进行观察 其次，确定算法 确定步骤 编程实现 常用的分类算法 KNN算法 贝克斯方法 决策树 人工神经网络 支持向量机（SVM） KNN算法 根据特征值建立维度，通过未知物品在这个维度中与其他物品的“距离”来分类 先分类，然后按照更群的距离对该对象进行归类 步骤 处理数据 数据向量化（处理到一个特征维度） 计算欧几里得距离（核心） 根据距离进行分类 from numpy import * import operator from os import listdir #这个数据集的行列代表什么，数值代表什么？ #这个数据集的每一行代表一个“物品”（不同两行可能代表相同“物品”） 每一列代表一个属性 其中的数值代表特征值 #(个数，测试数据集，训练数据集，分类名称) def knn(k,testdata,traindata,labels): #得到行数 tile是numpy里面的 traindatasize = traindata.shape[0] #(需扩展的数组，（行扩展倍数，列扩展倍数）) dif = tile(testdata,(traindata.size,1))-traindata sqdif = dif**2 #将每一行相加 sumsqdif = sqdif.sum(axis=1) dis = sumsqdif**0.5 #排序 得到下标序号 sortdis = dis.argsort() count = {} for i in range(0,k): vote = labels[sortdis[i]] count[vote] = count.get(vote,0)+1 sortcount = sorted(count.items(),key=operator.itemgetter(1),reverse=True) return sortcount[0][0] #读取文件变为一维数组 def datatoarray(fname): arr = [] fh = open(fname) for i in range(0,32): thisline = fh.readline() for j in range(0,32): arr.append(int(thisline[j])) return arr #取文件前缀，得到lables def seplabel(fname): filestr = fname.split(&quot;.&quot;)[0] label = int(filestr.split(&quot;_&quot;)[0]) return label #得到训练数组和标签数组 def traindata(): labels=[] trainfile = listdir(&quot;D:\\项目\\数据挖掘\\数据\\图片&quot;) num = len(trainfile) #长度1024（列），每一行存储一个文件 #用一个数组存储所有训练数据，行：文件总数，列：1024（32*32存储每一个像素点） trainarr = zeros((num,1024)) for i in range(0,num): thisfname = trainfile[i] thislabel = seplabel(thisfname) labels.append(thislabel) trainarr[i,:] = datatoarray(&quot;traindata/&quot;+thisfname)#赋予二维数组一行数据 return trainarr,labels 回归逻辑回归： 功能：判断是非问题，例如判断邮件是否为垃圾邮件 步骤： 在特征维度中拟合出一条线来 根据物品和线的关系得出结论 朴素贝叶斯贝叶斯分类： import numpy as npy import operator class Bayes: def __init__(self): self.length = -1 self.labelcount = dict() self.vectorcount = dict() #(已知物体，这些物体的类别) def fit(self,dataSet:list,labels:list): if(len(dataSet)!=len(labels)): return ValueError(&quot;输入的数据数和他们所属的类别数不一样&quot;) self.length = len(dataSet[0])#特征数数量 labelnum = len(labels) norlabels = set(labels) for item in norlabels: self.labelcount[item] = labels.count(item)/labelnum#计算每个类别的概率 #实现同时遍历两个列表 #将每个物体的特征数组存入对应的字典中 for vetor,label in zip(dataSet,labels): if(label not in self.vectorcount): self.vectorcount[label] = [] self.vectorcount[label].append(vetor) #计算出每个特征量在不同类中的概率 #1.行相加2.每一个特征值除以总行数 （记住每个特征值相互独立） for item in self.vectorcount: tem = npy.array(self.vectorcount[item]) rowsum = tem.shape[0]#行数 temcount = tem.sum(axis=0)#0不同行相加 1不同列相加 self.vectorcount[item] = temcount/rowsum print(&quot;训练完毕&quot;) return self def btest(self,testdata): if(self.length == -1): return ValueError(&quot;你还没有训练&quot;) tem = npy.array([]) pdict = {} for label in self.vectorcount: temp = 1 thisdata = self.vectorcount[label] thisp = self.labelcount[label] for item in range(len(testdata)): if testdata[item] == 1:temp = temp*thisdata[item] else: temp = temp*(1-thisdata[item]) temp = temp * thisp pdict[label] = temp sortcount = sorted(pdict.items(),key=operator.itemgetter(1),reverse=True) print(sortcount[0]) # for item in self.vectorcount: bayes = Bayes() dataSet = [[1,0],[1,0],[0,1],[0,1],[1,1]] labels = [&quot;学习好&quot;,&quot;学习好&quot;,&quot;运动好&quot;,&quot;运动好&quot;,&quot;学习好&quot;] bayes.fit(dataSet,labels) bayes.btest([1,0])","categories":[],"tags":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://xixfeng.github.io/tags/数据挖掘/"}],"keywords":[]},{"title":"数据库","slug":"数据库","date":"2018-11-20T15:42:08.000Z","updated":"2019-06-18T04:15:54.276Z","comments":true,"path":"2018/11/20/数据库/","link":"","permalink":"https://xixfeng.github.io/2018/11/20/数据库/","excerpt":"","text":"数据库三大范式 第一范式（1NF）：字段（列）不可再分 第二范式（2NF）：满足1NF,非主属性完全依赖于候选键 第三范式（3NF）：满足2NF,非主属性对候选键没有传递依赖 BCNF：满足3NF，且所有函数依赖对左边都有候选键 五大约束 primary KEY：设置主键 NOT NULL：不为NULL FOREIGN key：设置外键约束 unique：设计唯一性约束 default：默认 码 超码：能唯一确定一个元组的字段集合 候选码：超码的最小集合 主码：在多候选码中选一个作为主码，主码默认不为空，默认唯一性，默认自动增长 事物四大特性 原子性：事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 一致性：事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态 持久性：也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的 隔离性：一个事务的执行不能其它事务干扰 SQL 修改表 create table teacher(Id bigint,name varchar(255) not null, student_id int foreign key references student(id),primary key(Id))//常用的类型就是int，varchar(255) alter table teacher add (name varchar(255) not null) alter table teacher modify name int not null alter table teacher change name teacher_name varchar(255) not null alter table teacher drop teacher_name 查看表 show tables; desc teacher; 增删改 insert into teacher(Id,name) values(1,&quot;李小花&quot;),(2,&quot;李春花&quot;) update teacher set name=&quot;李春花&quot; where Id=1 delete from teacher where Id=1 自然连接natural join 会自动用两张表相同的字段进行连接 内部连接连接只是提供了一种常用的查询方法，将表联系起来，不用连接语法，用where也行 根据一个字段与其它表联系起来，查询两个表的字段. 并且只有当该字段相等时，才返回该行数据 不以哪个表为基础 SELECT ProductName, Quantity FROM Products INNER JOIN OrderDetails on OrderDetails.ProductID = Products.ProductID WHERE Quantity&lt;5 SELECT * FROM Orders INNER JOIN Customers on Customers.CustomerID = Orders.CustomerID SELECT COUNT(*) AS sum FROM Orders INNER JOIN Customers on Customers.CustomerID = Orders.CustomerID WHERE CustomerName = &quot;Around the Horn&quot; 三张表内连接 select * from student inner join takes on student.ID=takes.ID inner join course on takes.course_id=course.course_id where takes.ID=12345 外部连接左连接以左表为基础 SELECT * FROM Customers LEFT JOIN Orders on Customers.CustomerID = Orders.CustomerID 会返回数据库中所有的左表（Customers）的数据，右表如果匹配到了就返回，未匹配到就用NULL代替 全连接感觉很少用 SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons FULL JOIN Orders ON Persons.Id_P=Orders.Id_P ORDER BY Persons.LastName 不管两个表有没有互相匹配到，都会返回两个表中所有字段 in existshttps://blog.csdn.net/wqc19920906/article/details/79800374 #找到选课包含学生1全部课程的学生 #重点是利用exists的性质，一个一个的比较 select * from student as s1 where exists （ (select * from student_course where student.student_id = 1) excepts (select * from student as s2 where s1=s2) ) and not(student_id=1) 注意点 注意select选择字段时，不管后面是什么样子必须要加表名。 变为大小写不敏感select * from department where dept_name like lower(&#39;%sci%&#39;) 在sql语句中使用case when then else end得到条件结构 属性前面使用distinct去重 mysql嵌套子查询，需要在括号后面加一个表名.如果需要在这个临时关系中查询，则可以使用临时关系名.属性 group by rollup(c1,c2)会查询到根据(c1,null),(null,c2),(c1,c2)分组的结果(这三个叫分组集) 假如直接用group by(c1,c2)就是根据(c1,c2)分组 select publisher,member.memb_no from (select count(borrowed.date)as num,book.publisher,member.memb_no from member natural join book natural join borrowed group by book.publisher,member.memb_no)A where A.num&gt;5 当同时查询两种不相关的表时，会求笛卡尔积。一般插入时使用 select * from student,course where student.dept_name=&#39;Comp. Sci.&#39; and course.course_id=&#39;CS-001&#39; exists和in的区别，二者执行顺序不同，但是都是判断内外表之间存在管理,但是一般用in来联系内外表，用exists判断是否为空 关系代数并、交、差、笛卡尔积、选择、投影、更名 自然连接、除法、聚合函数 聚合函数在未分组的情况下，返回的是给的参数；在分组的情况下，返回的是分组字段和参数；聚合函数结果命名加as E-R图 可以有一个关联的最大和最小的映射基数 函数与存储过程 函数与过程可以在控制语句内加sql语句，或者在begin end中加sql语句 创建存储过程和函数CREATE PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_body CREATE FUNCTION sp_name ([func_parameter[,...]]) RETURNS type [characteristic ...] routine_body proc_parameter: [ IN | OUT | INOUT ] param_name type func_parameter: param_name type type: Any valid MySQL data type characteristic: LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT &#39;string&#39; routine_body: Valid SQL procedure statement or statements delimiter $$ #这里告诉编译器不要在;处结束 drop create function my_sum(x int) returns int begin set @i = 1;#要改变的话必须加@ set @sum = 0; while @i &lt;= x do set @sum = @sum + @i; set @i = @i + 1; end while; return @sum; end $$ delimiter ; 指定参数为IN， OUT， 或INOUT 只对PROCEDURE是合法的。（FUNCTION参数总是被认为是IN） 由于在过程中会涉及多条 SQL 语句，因此在程序被定义的时候，用 mysql 客户端 delimiter 命令来把语句定界符从 ; 改为 //，这样在语句体末尾开头加上 // 就表示存储过程结束。 存储过程和修改函数ALTER {PROCEDURE | FUNCTION} sp_name [characteristic ...] characteristic: { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT &#39;string&#39; 和创建一个语法，就是把create改成alter 声明变量显示声明 DECLARE var_name[,...] type [DEFAULT value] 隐示声明，变量可以是局部变量 全局变量 或没有被声明过的变量 SET var_name = expr 流程控制语句if IF search_condition THEN statement_list [ELSEIF search_condition THEN statement_list] ... [ELSE statement_list] END IF case CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list] END CASE Or: CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list] END CASE 循环控制 leave label iterate label#类似于continue loop leave [begin_label:] LOOP statement_list END LOOP [end_label]#begin_label和end_label必须同时出现 repeat [begin_label:] REPEAT statement_list UNTIL search_condition END REPEAT [end_label]#直到search_condition为真才结束 while [begin_label:] WHILE search_condition DO statement_list END WHILE [end_label] 由于存储过程语句经过了预编译，执行速度也会有较大提升，适合用于一些频繁的操作。 光标光标的声明要在变量之后 DECLARE cursor_name CURSOR FOR select_statement OPEN cursor_name #打开声明的光标 FETCH cursor_name #移动光标到下一行并读取 Close cursor_name #关闭光标 栗子create function avg_salary(company_name varchar(50)) returns integer declear res integer select avg(salary) into res from works where company_name = company_name return res end select company name from works where avg_salary(company_name) &gt; avg salary(&quot;First Bank Corporation&quot;) 触发器 触发器和存储过程差不多，能使用控制语句，sql的使用方法也与过程相同具体语法和例子看电子书笔记 update，insert一般用new row，delete一般用old row new row获得修改发生后的被插入或更新元组，old row获得修改发生前将被修改、更新、删除的元组","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://xixfeng.github.io/tags/数据库/"}],"keywords":[]},{"title":"java集合","slug":"java集合","date":"2018-11-20T03:10:26.000Z","updated":"2018-11-20T03:50:54.000Z","comments":true,"path":"2018/11/20/java集合/","link":"","permalink":"https://xixfeng.github.io/2018/11/20/java集合/","excerpt":"","text":"","categories":[],"tags":[],"keywords":[]},{"title":"测试","slug":"测试","date":"2018-11-20T02:37:47.000Z","updated":"2019-06-15T12:48:08.700Z","comments":true,"path":"2018/11/20/测试/","link":"","permalink":"https://xixfeng.github.io/2018/11/20/测试/","excerpt":"","text":"测试转自 TDD 有三层含义： Test-Driven Development，测试驱动开发。 Task-Driven Development，任务驱动开发，要对问题进行分析并进行任务分解。 Test-Driven Design，测试保护下的设计改善。TDD 并不能直接提高设计能力，它只是给你更多机会和保障去改善设计。 为什么要 TDD传统编码方式 VS TDD 编码方式 传统编码方式 需求分析，想不清楚细节，管他呢，先开始写 发现需求细节不明确，去跟业务人员确认 确认好几次终于写完所有逻辑 运行起来测试一下，靠，果然不工作，调试 调试好久终于工作了 转测试，QA 测出 bug，debug， 打补丁 终于，代码可以工作了 一看代码烂的像坨屎，不敢动，动了还得手工测试，还得让 QA 测试，还得加班… TDD 编码方式 先分解任务，分离关注点（分解成一个个小函数） 写测试，只关注需求，程序的输入输出，不关心中间过程（比如用例中，测试就是文本内容） 写实现，不考虑别的需求，用最简单的方式满足当前这个小需求即可 重构，用手法消除代码里的坏味道 写完，手动测试一下，基本没什么问题，有问题补个用例，修复 转测试，小问题，补用例，修复 代码整洁且用例齐全，信心满满地提交 只测试自己没有信心的代码部分· 例子写一个程序来计算一个文本文件 words.txt 中每个单词出现的频率。 为了保持简单，假设： words.txt 只包含小写字母和空格 每个单词只包含小写字母 单词之间由一个或多个空格分开 举个例子，假设 words.txt 包含以下内容： the day is sunny the the the sunny is is 你的程序应当输出如下，按频率倒序排序： the 4 is 3 sunny 2 day 1 请先不要往下读，思考一下你会怎么做。 （思考 3 分钟…） 新手拿到这样的需求呢，就会把所有代码写到一个 main() 方法里，伪代码如下： main() { // 读取文件 ... // 分隔单词 ... // 分组 ... // 倒序排序 ... // 拼接字符串 ... // 打印 ... } 思路很清晰，但往往一口气写完，最后运行起来，输出却不符合预期，然后就开始打断点调试。 这种代码没有任何的封装。这就是为什么很多人一听到说有些公司限制一个方法不超过 10 行，就立马跳出来说，这不可能，10 行能干什么啊，我们的业务逻辑很复杂… 这样的代码存在什么样的问题呢？ 不可测试 不可重用 难以定位问题 好嘛，那我们来 TDD 嘛，你说读文件，输出控制台的测试代码要怎么写？ 当然，我们可以通过 Mock 和 Stub 来隔离 IO，但真的有必要吗？ 有人问过 Kent Beck 这样一个问题： 你真的什么都会测吗？连 getter 和 setter 也会测试吗？ Kent Beck 说：公司请我来是为了实现业务价值，而不是写测试代码。 所以我只在没有信心的地方写测试代码。 那对我们这个程序而言，读文件和打印到控制台都是调用系统 API，可以很有信心吧。最没有信心的是中间那写要自己写的业务逻辑。 所以我们可以对程序做一些封装，《代码整洁之道》里说，有注释的地方都可以抽取方法，用方法名来代替注释： main() { String words = read_file(&#39;words.txt&#39;) String[] wordArray = split(words) Map&lt;String, Integer&gt; frequency = group(wordArray) sort(frequency) String output = format(frequency) print(output) } 这样是不是就可以单独为 split，group，sort，format 这些方法写单元测试了呢？ 当然可以， 它们的输入和输出都是很明确的嘛。 等等，你可能会说，不是测试驱动设计吗？你怎么开始做设计了？好问题！ TDD 要不要做提前设计呢？ Kent Beck 不做提前设计，他会选一个最简单的用例，直接开写，用最简单的代码通过测试。逐渐增加测试，让代码变复杂，用重构来驱动出设计。 在这个需求里，最简单的场景是什么呢？ 那就是文件内容为空，输出也为空。 当然，对于复杂问题，可能要一边写一边补充新的用例，但对于这种简单的题目，基本可以提前就想清楚用什么用例驱动去什么产品代码。 大概可以想到如下的用例： “” =&gt; “” “he” =&gt; “he 1”，一个单词，驱动出格式化字符串的代码 “he is” =&gt; “he 1\\r\\nis 1”，两个不同单词，驱动出分割单词的代码 “he he is” =&gt; “he 2\\r\\nis 1”，有相同单词，驱动出分组代码 “he is is” =&gt; “is 2\\r\\nhe 1”，驱动出分组后的排序代码 “he is” =&gt; “he 1\\r\\nis 1”，多个空格，完善分割单词的代码 java单元测试（junity）转自 IDEA添加测试用例，CTRL+Shift+t @BeforeClass 全局只会执行一次，而且是第一个运行 @Before 在测试方法运行之前运行 @Test(timeout = 1000) 测试方法，设置测试时间（防止死循环） @After 在测试方法运行之后允许 @AfterClass 全局只会执行一次，而且是最后一个运行 @Ignore(“Multiply() Not yet implemented”) 参数化测试我们可能遇到过这样的函数，它的参数有许多特殊值，或者说他的参数分为很多个区域。例如，测试一下“计算一个数的平方”这个函数，暂且分三类：正数、0、负数。在编写测试的时候，至少要写3个测试，把这3种情况都包含了，这确实是一件很麻烦的事情。测试代码如下： public class AdvancedTest { private static Calculator calculator ＝ new Calculator(); @Before public void clearCalculator(){ calculator.clear(); } @Test public void square1() { calculator.square(2); assertEquals(4, calculator.getResult()); } @Test public void square2(){ calculator.square(0); assertEquals(0, calculator.getResult()); } @Test public void square3(){ calculator.square(-3); assertEquals(9, calculator.getResult()); } } 为了简化类似的测试，JUnit4提出了“参数化测试”的概念，只写一个测试函数，把这若干种情况作为参数传递进去，一次性的完成测试。代码如下： @RunWith(Parameterized.class) public class SquareTest{ private static Calculator calculator = new Calculator(); private int param; private int result; @Parameters public static Collection data() { return Arrays.asList(new Object[][]{ {2, 4}, {0, 0}, {-3, 9}, }); }//@RunWith 和 @Parameter 注解用于为单元测试提供参数值，@Parameters必须返回 List，参数将会被作为参数传给类的构造函数。 //构造函数，对变量进行初始化 public SquareTest(int param, int result){ this.param = param; this.result = result; } @Test public void square(){ calculator.square(param); assertEquals(result, calculator.getResult()); } } 执行了3次该测试类，依次采用了数据集合中的数据{处理值，预期处理结果}，结果如下：代码分析如下： 为这种测试专门生成一个新的类，而不能与其他测试共用同一个类，此例中我们定义了一个SquareTest类。 为这个类指定一个Runner，而不能使用默认的Runner，@RunWith(Parameterized.class)这条语句就是为这个类指定了一个ParameterizedRunner 定义一个待测试的类，并且定义两个变量，一个用于存放参数，一个用于存放期待的结果。 定义测试数据的集合，也就是上述的data()方法，该方法可以任意命名，但是必须使用@Parameters标注进行修饰。 定义构造函数，其功能就是对先前定义的两个参数进行初始化 测试工具 决定用JMeter来做压力测试，ab简单测试 JMeter https://blog.csdn.net/zl1zl2zl3/article/details/78194194 ab","categories":[],"tags":[{"name":"测试","slug":"测试","permalink":"https://xixfeng.github.io/tags/测试/"}],"keywords":[]},{"title":"github后端架构师技术图谱","slug":"github后端架构师技术图谱","date":"2018-11-17T04:06:28.000Z","updated":"2019-06-12T08:11:40.912Z","comments":true,"path":"2018/11/17/github后端架构师技术图谱/","link":"","permalink":"https://xixfeng.github.io/2018/11/17/github后端架构师技术图谱/","excerpt":"","text":"压力测试apache ab 并发并发知识合集 基础知识1.同步、异步：同步方法调用一开始，调用者必须等待被调用的方法结束后，调用者后面的代码才能执行。而异步调用，指的是，调用者不用管被调用方法是否完成，都会继续执行后面的代码。 2.并发并行：并发和并行是十分容易混淆的概念。并发指的是多个任务交替进行，而并行则是指真正意义上的“同时进行”。实际上，如果系统内只有一个CPU，而使用多线程时，那么真实系统环境下不能并行，只能通过切换时间片的方式交替进行，而成为并发执行任务。真正的并行也只能出现在拥有多个CPU的系统中。 3.阻塞和非阻塞：阻塞和非阻塞通常用来形容多线程间的相互影响，比如一个线程占有了临界区资源，那么其他线程需要这个资源就必须进行等待该资源的释放，会导致等待的线程挂起，这种情况就是阻塞，而非阻塞就恰好相反，它强调没有一个线程可以阻塞其他线程，所有的线程都会尝试地往前运行。线程能否阻塞其他线程 4.临界区 临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用。但是每个线程使用时，一旦临界区资源被一个线程占有，那么其他线程必须等待 线程状态 此图来源于《JAVA并发编程的艺术》一书中，线程是会在不同的状态间进行转换的，java线程线程转换图如上图所示。线程创建之后调用start()方法开始运行，当调用wait(),join(),LockSupport.lock()方法线程会进入到WAITING状态，而同样的wait(long timeout)，sleep(long),join(long),LockSupport.parkNanos(),LockSupport.parkUtil()增加了超时等待的功能，也就是调用这些方法后线程会进入TIMED_WAITING状态，当超时等待时间到达后，线程会切换到Runable的状态，另外当WAITING和TIMED _WAITING状态时可以通过Object.notify(),Object.notifyAll()方法使线程转换到Runable状态。当线程出现资源竞争时，即等待获取锁的时候，线程会进入到BLOCKED阻塞状态，当线程获取锁时，线程进入到Runable状态。线程运行结束后，线程进入到TERMINATED状态，状态转换可以说是线程的生命周期。另外需要注意的是： 当线程进入到synchronized方法或者synchronized代码块时，线程切换到的是BLOCKED状态，而使用java.util.concurrent.locks下lock进行加锁的时候线程切换的是WAITING或者TIMED_WAITING状态，因为lock会调用LockSupport的方法。 用一个表格将上面六种状态进行一个总结归纳。 java线程操作 interrupted join(Thread thread1)，等待thread1线程执行完毕后执行 sleep public static native void sleep(long millis)方法显然是Thread的静态方法，很显然它是让当前线程按照指定的时间休眠，其休眠时间的精度取决于处理器的计时器和调度器。需要注意的是如果当前线程获得了锁，sleep方法并不会失去锁。sleep方法经常拿来与Object.wait()方法进行比价，这也是面试经常被问的地方。 sleep() VS wait() 两者主要的区别： sleep()方法是Thread的静态方法，而wait是Object实例方法 wait()方法必须要在同步方法或者同步块中调用，也就是必须已经获得对象锁。而sleep()方法没有这个限制可以在任何地方种使用。另外，wait()方法会释放占有的对象锁，使得该线程进入等待池中，等待下一次获取资源。而sleep()方法只是会让出CPU并不会释放掉对象锁； sleep()方法在休眠时间达到后如果再次获得CPU时间片就会继续执行，而wait()方法必须等待Object.notift/Object.notifyAll通知后，才会离开等待池，并且再次获得CPU时间片才会继续执行。 yield public static native void yield();这是一个静态方法，一旦执行，它会是当前线程让出CPU，但是，需要注意的是，让出的CPU并不是代表当前线程不再运行了，如果在下一次竞争中，又获得了CPU时间片当前线程依然会继续运行。另外，让出的时间片只会分配给当前线程相同优先级的线程。什么是线程优先级了？下面就来具体聊一聊。 并发理论（JMM）并发理论（JMM） java内存模型以及happens-before规则 知识点：（1）JMM内存结构；（2）重排序；（3）happens-before规则 我们知道CPU的处理速度和主存的读写速度不是一个量级的，为了平衡这种巨大的差距，每个CPU都会有缓存。因此，共享变量会先放在主存中，每个线程都有属于自己的工作内存，并且会把位于主存中的共享变量拷贝到自己的工作内存，之后的读写操作均使用位于工作内存的变量副本，并在某个时刻将工作内存的变量副本写回到主存中去。JMM就从抽象层次定义了这种方式，并且JMM决定了一个线程对共享变量的写入何时对其他线程是可见的。即JMM管理了线程对数据的读写 我的理解就是：JMM理论的实现，就是在内存操作与程序员之间建立了一层模型，减轻程序员的编程负担 即关键词的实现是建立在JMM理论之上的 并发关键字synchronized（并发） 锁是通过类的监视器monitor来实现 锁的优化：1.CAS乐观锁策略2.对象头 CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程 CAS的实现需要硬件指令集的支撑，在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现。 Synchronized VS CAS 元老级的Synchronized(未优化前)最主要的问题是：在存在线程竞争的情况下会出现线程阻塞和唤醒锁带来的性能问题，因为这是一种互斥同步（阻塞同步）。而CAS并不是武断的间线程挂起，当CAS操作失败后会进行一定的尝试，而非进行耗时的挂起唤醒的操作，因此也叫做非阻塞同步。这是两者主要的区别。 按照我目前的理解就是多个线程去操作同一个值的时候，只有一个能成功，其余的都会失败，既然这样为什么会失败过后重新尝试呢？尝试不是照样会失败吗 看到了对象头，暂时放放。先了解后端表面的东西 HTTP协议状态码：206：表示服务器已经处理部分Get请求 设计模式mvcModel（模型）：模型代表一个存取数据的对象或 JAVA POJO。他也可以带有逻辑。 View（视图）：视图代表数据的可视化 Controller（控制器）：他控制数据流向模型对象，并在数据变化时更新视图。他使视图与模型分离开。 iocIoC：控制反转，改变上层依赖下层的现状 spring中ioc：ioc的思想最核心的地方在于，资源不由使用资源的双方管理，而由不使用资源的第三方管理，这可以带来很多好处。第一，资源集中管理，实现资源的可配置和易管理。第二，降低了使用资源双方的依赖程度，也就是我们说的耦合度。 实现，通过依赖注入，即上层，底层都依赖第三方，实现上层类对下层类的控制，改变上层对下层的直接依赖关系 也就是说，甲方要达成某种目的不需要直接依赖乙方，它只需要达到的目的告诉第三方机构就可以了，比如甲方需要一双袜子，而乙方它卖一双袜子，它要把袜子卖出去，并不需要自己去直接找到一个卖家来完成袜子的卖出。它也只需要找第三方，告诉别人我要卖一双袜子。这下好了，甲乙双方进行交易活动，都不需要自己直接去找卖家，相当于程序内部开放接口，卖家由第三方作为参数传入。甲乙互相不依赖，而且只有在进行交易活动的时候，甲才和乙产生联系。反之亦然。这样做什么好处么呢，甲乙可以在对方不真实存在的情况下独立存在，而且保证不交易时候无联系，想交易的时候可以很容易的产生联系。甲乙交易活动不需要双方见面，避免了双方的互不信任造成交易失败的问题。因为交易由第三方来负责联系，而且甲乙都认为第三方可靠。那么交易就能很可靠很灵活的产生和进行了。 这就是ioc的核心思想。生活中这种例子比比皆是，支付宝在整个淘宝体系里就是庞大的ioc容器，交易双方之外的第三方，提供可靠性可依赖可灵活变更交易方的资源管理中心。另外人事代理也是，雇佣机构和个人之外的第三方。嗯，就这样，希望对题主有帮助。 依赖注入：●谁依赖于谁：当然是应用程序依赖于IoC容器； ●为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源； ●谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； ●注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 AOPAOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。不过OOP允许开发者定义纵向的关系，但并不适合定义横向的关系，例如日志功能。日志代码往往横向地散布在所有对象层次中，而与它对应的对象的核心功能毫无关系对于其他类型的代码，如安全性、异常处理和透明的持续性也都是如此，这种散布在各处的无关的代码被称为横切（cross cutting），在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 AOP技术恰恰相反，它利用一种称为”横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为”Aspect”，即切面。所谓”切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。 使用”横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事物。AOP的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。 AOP核心概念 1、横切关注点 对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点 2、切面（aspect） 类是对物体特征的抽象，切面就是对横切关注点的抽象 3、连接点（joinpoint） 被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器 4、切入点（pointcut） 对连接点进行拦截的定义 5、通知（advice） 所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类 6、目标对象 代理的目标对象 7、织入（weave） 将切面应用到目标对象并导致代理对象创建的过程 8、引入（introduction） 在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段 servlet广义的servlet就是实现了java Servlet接口 servlet接口：处理请求以及响应 Tomcat：servlet容器（处理http协议） Springmvc SpringBoot详细讲解 Boot解决了mvc繁琐配置的问题 boot中 application.properties出现中文乱码问题，当时尝试了很多办法都没有解决。Spring Boot 总是会以iso-8859的编码方式读取该文件，后来改用 YAML 了就再也没有出现过乱码了。并且它也拥有更简洁的语法，所以在此也更推荐使用application.yml作为默认的配置文件。 boot 约定大于配置：其他人根据编程习惯实现了一部分代码功能，我们只需要使用。（就是能不变就不变的东西别人已经替我们实现好了） 网络加速cdncontent delivery network内容分发网络 缓存是什么？ 这里不深究CDN背后高大上的架构，也不讨论CDN如何做到全局流量调度策略，本文着重讨论在有了CDN后，数据是如何被缓存的。缓存是一个到处都存在的用空间换时间的例子。通过使用多余的空间，我们能够获取更快的速度。 首先，看看没有网站没有接入CDN时，用户浏览器与服务器是如何交互的： 用户在浏览网站的时候，浏览器能够在本地保存网站中的图片或者其他文件的副本，这样用户再次访问该网站的时候，浏览器就不用再下载全部的文件，减少了下载量意味着提高了页面加载的速度。 如果中间加上一层CDN，那么用户浏览器与服务器的交互如下： ​ 客户端浏览器先检查是否有本地缓存是否过期，如果过期，则向CDN边缘节点发起请求，CDN边缘节点会检测用户请求数据的缓存是否过期，如果没有过期，则直接响应用户请求，此时一个完成http请求结束；如果数据已经过期，那么CDN还需要向源站发出回源请求（back to the source request）,来拉取最新的数据。CDN的典型拓扑图如下： 图片来源：http://grefr.iteye.com/blog/2004248 可以看到，在存在CDN的场景下，数据经历了客户端（浏览器）缓存和CDN边缘节点缓存两个阶段 论云计算、大数据、人工智能转自 一、云计算最初是实现资源管理的灵活性我们首先来说云计算，云计算最初的目标是对资源的管理，管理的主要是计算资源，网络资源，存储资源三个方面。 1.1 管数据中心就像配电脑 什么叫计算，网络，存储资源呢？就说你要买台笔记本电脑吧，你是不是要关心这台电脑什么样的CPU啊？多大的内存啊？这两个我们称为计算资源。 这台电脑要能上网吧，需要有个网口可以插网线，或者有无线网卡可以连接我们家的路由器，您家也需要到运营商比如联通，移动，电信开通一个网络，比如100M的带宽，然后会有师傅弄一根网线到您家来，师傅可能会帮您将您的路由器和他们公司的网络连接配置好，这样您家的所有的电脑，手机，平板就都可以通过您的路由器上网了。这就是网络。 您可能还会问硬盘多大啊？原来硬盘都很小，10G之类的，后来500G，1T，2T的硬盘也不新鲜了。(1T是1024G)，这就是存储。 对于一台电脑是这个样子的，对于一个数据中心也是同样的。想象你有一个非常非常大的机房，里面堆了很多的服务器，这些服务器也是有CPU，内存，硬盘的，也是通过类似路由器的设备上网的。这个时候的一个问题就是，运营数据中心的人是怎么把这些设备统一的管理起来的呢？ 1.2 灵活就是想啥时要都有，想要多少都行 管理的目标就是要达到两个方面的灵活性。哪两个方面呢？比如有个人需要一台很小很小的电脑，只有一个CPU，1G内存，10G的硬盘，一兆的带宽，你能给他吗？像这种这么小规格的电脑，现在随便一个笔记本电脑都比这个配置强了，家里随便拉一个宽带都要100M。然而如果去一个云计算的平台上，他要想要这个资源的时候，只要一点就有了。 所以说它就能达到两个方面灵活性。 第一个方面就是想什么时候要就什么时候要，比如需要的时候一点就出来了，这个叫做时间灵活性。 第二个方面就是想要多少呢就有多少，比如需要一个很小很小的电脑，可以满足，比如需要一个特别大的空间，以云盘为例，似乎云盘给每个人分配的空间动不动就就很大很大，随时上传随时有空间，永远用不完，这个叫做空间灵活性。 空间灵活性和时间灵活性，也即我们常说的云计算的弹性。 为了解决这个弹性的问题，经历了漫长时间的发展。 1.3 物理设备不灵活 首先第一个阶段就是物理机，或者说物理设备时期。这个时期相当于客户需要一台电脑，我们就买一台放在数据中心里。物理设备当然是越来越牛，例如服务器，内存动不动就是百G内存，例如网络设备，一个端口的带宽就能有几十G甚至上百G，例如存储，在数据中心至少是PB级别的(一个P是1024个T，一个T是1024个G)。 然而物理设备不能做到很好的灵活性。首先它不能够达到想什么时候要就什么时候要、比如买台服务器，哪怕买个电脑，都有采购的时间。突然用户告诉某个云厂商，说想要开台电脑，如果使用物理服务器，当时去采购啊就很难，如果说供应商啊关系一般，可能采购一个月，供应商关系好的话也需要一个星期。用户等了一个星期后，这时候电脑才到位，用户还要登录上去开始慢慢部署自己的应用，时间灵活性非常差。第二是空间灵活性也不行，例如上述的用户，要一个很小很小的电脑，现在哪还有这么小型号的电脑啊。不能为了满足用户只要一个G的内存是80G硬盘的，就去买一个这么小的机器。但是如果买一个大的呢，因为电脑大，就向用户多收钱，用户说他只用这么小的一点，如果让用户多付钱就很冤。 1.4 虚拟化灵活多了 有人就想办法了。第一个办法就是虚拟化。用户不是只要一个很小的电脑么？数据中心的物理设备都很强大，我可以从物理的CPU，内存，硬盘中虚拟出一小块来给客户，同时也可以虚拟出一小块来给其他客户，每个客户都只能看到自己虚的那一小块，其实每个客户用的是整个大的设备上其中的一小块。虚拟化的技术能使得不同的客户的电脑看起来是隔离的，我看着好像这块盘就是我的，你看这呢这块盘就是你的，实际情况可能我这个10G和您这个10G是落在同样一个很大很大的这个存储上的。 而且如果事先物理设备都准备好，虚拟化软件虚拟出一个电脑是非常快的，基本上几分钟就能解决。所以在任何一个云上要创建一台电脑，一点几分钟就出来了，就是这个道理。 这个空间灵活性和时间灵活性就基本解决了。 1.5 虚拟世界的赚钱与情怀 在虚拟化阶段，最牛的公司是Vmware，是实现虚拟化技术比较早的一家公司，可以实现计算，网络，存储的虚拟化，这家公司很牛，性能也做得非常好，然后虚拟化软件卖的也非常好，赚了好多的钱，后来让EMC(世界五百强，存储厂商第一品牌)给收购了。 但是这个世界上还是有很多有情怀的人的，尤其是程序员里面，有情怀的人喜欢做一件什么事情呢？开源。这个世界上很多软件都是有闭源就有开源，源就是源代码。就是说某个软件做的好，所有人都爱用，这个软件的代码呢，我封闭起来只有我公司知道，其他人不知道，如果其他人想用这个软件，就要付我钱，这就叫闭源。但是世界上总有一些大牛看不惯钱都让一家赚了去。大牛们觉得，这个技术你会我也会，你能开发出来，我也能，我开发出来就是不收钱，把代码拿出来分享给大家，全世界谁用都可以，所有的人都可以享受到好处，这个叫做开源。 比如最近蒂姆·伯纳斯·李就是个非常有情怀的人，2017年，他因“发明万维网、第一个浏览器和使万维网得以扩展的基本协议和算法”而获得2016年度的图灵奖。图灵奖就是计算机界的诺贝尔奖。然而他最令人敬佩的是，他将万维网，也就是我们常见的www的技术无偿贡献给全世界免费使用。我们现在在网上的所有行为都应该感谢他的功劳，如果他将这个技术拿来收钱，应该和比尔盖茨差不多有钱。 例如在闭源的世界里有windows，大家用windows都得给微软付钱，开源的世界里面就出现了Linux。比尔盖茨靠windows，Office这些闭源的软件赚了很多钱，称为世界首富，就有大牛开发了另外一种操作系统Linux。很多人可能没有听说过Linux，很多后台的服务器上跑的程序都是Linux上的，比如大家享受双十一，支撑双十一抢购的系统，无论是淘宝，京东，考拉，都是跑在Linux上的。 再如有apple就有安卓。apple市值很高，但是苹果系统的代码我们是看不到的。于是就有大牛写了安卓手机操作系统。所以大家可以看到几乎所有的其他手机厂商，里面都装安卓系统，因为苹果系统不开源，而安卓系统大家都可以用。 在虚拟化软件也一样，有了Vmware，这个软件非常非常的贵。那就有大牛写了两个开源的虚拟化软件，一个叫做Xen，一个叫做KVM，如果不做技术的，可以不用管这两个名字，但是后面还是会提到。 1.6 虚拟化的半自动和云计算的全自动 虚拟化软件似乎解决了灵活性问题，其实不全对。因为虚拟化软件一般创建一台虚拟的电脑，是需要人工指定这台虚拟电脑放在哪台物理机上的，可能还需要比较复杂的人工配置，所以使用Vmware的虚拟化软件，需要考一个很牛的证书，能拿到这个证书的人，薪资是相当的高，也可见复杂程度。所以仅仅凭虚拟化软件所能管理的物理机的集群规模都不是特别的大，一般在十几台，几十台，最多百台这么一个规模。这一方面会影响时间灵活性，虽然虚拟出一台电脑的时间很短，但是随着集群规模的扩大，人工配置的过程越来越复杂，越来越耗时。另一方面也影响空间灵活性，当用户数量多的时候，这点集群规模，还远达不到想要多少要多少的程度，很可能这点资源很快就用完了，还得去采购。所以随着集群的规模越来越大，基本都是千台起步，动辄上万台，甚至几十上百万台，如果去查一下BAT，包括网易，包括谷歌，亚马逊，服务器数目都大的吓人。这么多机器要靠人去选一个位置放这台虚拟化的电脑并做相应的配置，几乎是不可能的事情，还是需要机器去做这个事情。 人们发明了各种各样的算法来做这个事情，算法的名字叫做调度(Scheduler)。通俗一点的说，就是有一个调度中心，几千台机器都在一个池子里面，无论用户需要多少CPU，内存，硬盘的虚拟电脑，调度中心会自动在大池子里面找一个能够满足用户需求的地方，把虚拟电脑启动起来做好配置，用户就直接能用了。这个阶段，我们称为池化，或者云化，到了这个阶段，才可以称为云计算，在这之前都只能叫虚拟化。（虚拟机的自动化分配） 1.7私有云，公有云 私有云：自己用的云计算 公有云：让大众使用云计算，如腾讯云，阿里云 1.8 云计算的赚钱与情怀 公有云的第一名亚马逊过得很爽，第二名Rackspace过的就一般了。没办法，这就是互联网行业的残酷性，多是赢者通吃的模式。所以第二名如果不是云计算行业的，很多人可能都没听过了。第二名就想，我干不过老大怎么办呢？开源吧。如上所述，亚马逊虽然使用了开源的虚拟化技术，但是云化的代码是闭源的，很多想做又做不了云化平台的公司，只能眼巴巴的看着亚马逊挣大钱。Rackspace把源代码一公开，整个行业就可以一起把这个平台越做越好，兄弟们大家一起上，和老大拼了。 于是Rackspace和美国航空航天局合作创办了开源软件OpenStack，如图所示OpenStack的架构图，不是云计算行业的不用弄懂这个图，但是能够看到三个关键字，Compute计算，Networking网络，Storage存储。还是一个计算，网络，存储的云化管理平台。 当然第二名的技术也是非常棒的，有了OpenStack之后，果真像Rackspace想象的一样，所有想做云的大企业都疯了，你能想象到的所有如雷贯耳的大型IT企业，IBM，惠普，戴尔，华为，联想等等，都疯了。原来云平台大家都想做，看着亚马逊和Vmware赚了这么多钱，眼巴巴看着没办法，想自己做一个好像难度还挺大。现在好了，有了这样一个开源的云平台OpenStack，所有的IT厂商都加入到这个社区中来，对这个云平台进行贡献，包装成自己的产品，连同自己的硬件设备一起卖。有的做了私有云，有的做了公有云，OpenStack已经成为开源云平台的事实标准。 1.9 IaaS, 资源层面的灵活性 随着OpenStack的技术越来越成熟，可以管理的规模也越来越大，并且可以有多个OpenStack集群部署多套，比如北京部署一套，杭州部署两套，广州部署一套，然后进行统一的管理。这样整个规模就更大了。在这个规模下，对于普通用户的感知来讲，基本能够做到想什么时候要就什么什么药，想要多少就要多少。还是拿云盘举例子，每个用户云盘都分配了5T甚至更大的空间，如果有1亿人，那加起来空间多大啊。其实背后的机制是这样的，分配你的空间，你可能只用了其中很少一点，比如说它分配给你了5个T，这么大的空间仅仅是你看到的，而不是真的就给你了，你其实只用了50个G，则真实给你的就是50个G，随着你文件的不断上传，分给你的空间会越来越多。当大家都上传，云平台发现快满了的时候(例如用了70%)，会采购更多的服务器，扩充背后的资源，这个对用户是透明的，看不到的，从感觉上来讲，就实现了云计算的弹性。其实有点像银行，给储户的感觉是什么时候取钱都有，只要不同时挤兑，银行就不会垮。 这里做一个简单的总结，到了这个阶段，云计算基本上实现了时间灵活性和空间灵活性，实现了计算，网络，存储资源的弹性。计算，网络，存储我们常称为基础设施Infranstracture, 因而这个阶段的弹性称为资源层面的弹性，管理资源的云平台，我们称为基础设施服务，就是我们常听到的IaaS，Infranstracture As A Service。 二、云计算不光管资源，也要管应用 有了IaaS，实现了资源层面的弹性就够了吗？显然不是。还有应用层面的弹性。这里举个例子，比如说实现一个电商的应用，平时十台机器就够了，双十一需要一百台。你可能觉得很好办啊，有了IaaS，新创建九十台机器就可以了啊。但是90台机器创建出来是空的啊，电商应用并没有放上去啊，只能你公司的运维人员一台一台的弄，还是需要很长时间才能安装好的。虽然资源层面实现了弹性，但是没有应用层的弹性，依然灵活性是不够的。 有没有方法解决这个问题呢？于是人们在IaaS平台之上又加了一层，用于管理资源以上的应用弹性的问题，这一层通常称为PaaS（Platform As A Service）。这一层往往比较难理解，其实大致分两部分，一部分我称为你自己的应用自动安装，一部分我称为通用的应用不用安装。 我们先来说第一部分，自己的应用自动安装。比如电商应用是你自己开发的，除了你自己，其他人是不知道怎么安装的，比如电商应用，安装的时候需要配置支付宝或者微信的账号，才能别人在你的电商上买东西的时候，付的钱是打到你的账户里面的，除了你，谁也不知道，所以安装的过程平台帮不了忙，但是能够帮你做的自动化，你需要做一些工作，将自己的配置信息融入到自动化的安装过程中方可。比如上面的例子，双十一新创建出来的90台机器是空的，如果能够提供一个工具，能够自动在这新的90台机器上将电商应用安装好，就能够实现应用层面的真正弹性。例如Puppet, Chef, Ansible, Cloud Foundary都可以干这件事情，最新的容器技术Docker能更好的干这件事情，不做技术的可以不用管这些词。 第二部分，通用的应用不用安装。所谓通用的应用，一般指一些复杂性比较高，但是大家都在用的，例如数据库。几乎所有的应用都会用数据库，但是数据库软件是标准的，虽然安装和维护比较复杂，但是无论谁安装都是一样。这样的应用可以变成标准的PaaS层的应用放在云平台的界面上。当用户需要一个数据库的时候，一点就出来了，用户就可以直接用了。有人问，既然谁安装都一个样，那我自己来好了，不需要花钱在云平台上买。当然不是，数据库是一个非常难的东西，光Oracle这家公司，靠数据库就能赚这么多钱。买Oracle也是要花很多很多钱的。然而大多数云平台会提供Mysql这样的开源数据库，又是开源，钱不需要花这么多了，但是维护这个数据库，却需要专门招一个很大的团队，如果这个数据库能够优化到能够支撑双十一，也不是一年两年能够搞定的。比如您是一个做单车的，当然没必要招一个非常大的数据库团队来干这件事情，成本太高了，应该交给云平台来做这件事情，专业的事情专业的人来自，云平台专门养了几百人维护这套系统，您只要专注于您的单车应用就可以了。 要么是自动部署，要么是不用部署，总的来说就是应用层你也要少操心，这就是PaaS层的重要作用。 虽说脚本的方式能够解决自己的应用的部署问题，然而不同的环境千差万别，一个脚本往往在一个环境上运行正确，到另一个环境就不正确了。 而容器是能更好的解决这个问题的。 容器是 Container，Container另一个意思是集装箱，其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是封装，二是标准。 在没有集装箱的时代，假设将货物从 A运到 B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，摆的七零八落，然后搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能走。 有了集装箱以后，所有的货物都打包在一起了，并且集装箱的尺寸全部一致，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用上岸长时间耽搁了。 这是集装箱“封装”、“标准”两大特点在生活中的应用。 那么容器如何对应用打包呢？还是要学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。好在 Ubuntu中的LXC技术早就能做到这一点。 封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为 Namespace，也即每个 Namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为 Cgroups，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。 所谓的镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定”，集装箱里面就定在了那一刻，然后将这一刻的状态保存成一系列文件。这些文件的格式是标准的，谁看到这些文件都能还原当时定住的那个时刻。将镜像还原成运行时的过程（就是读取镜像文件，还原那个时刻的过程）就是容器运行的过程。 有了容器，使得 PaaS层对于用户自身应用的自动部署变得快速而优雅。 三、大数据拥抱云计算在PaaS层中一个复杂的通用应用就是大数据平台。大数据是如何一步一步融入云计算的呢？ 3.1 数据不大也包含智慧 一开始这个大数据并不大，你想象原来才有多少数据？现在大家都去看电子书，上网看新闻了，在我们80后小时候，信息量没有那么大，也就看看书，看看报，一个星期的报纸加起来才有多少字啊，如果你不在一个大城市，一个普通的学校的图书馆加起来也没几个书架，是后来随着信息化的到来，信息才会越来越多。 首先我们来看一下大数据里面的数据，就分三种类型，一种叫结构化的数据，一种叫非结构化的数据，还有一种叫半结构化的数据。什么叫结构化的数据呢？叫有固定格式和有限长度的数据。例如填的表格就是结构化的数据，国籍：中华人民共和国，民族：汉，性别：男，这都叫结构化数据。现在越来越多的就是非结构化的数据，就是不定长，无固定格式的数据，例如网页，有时候非常长，有时候几句话就没了，例如语音，视频都是非结构化的数据。半结构化数据是一些xml或者html的格式的，不从事技术的可能不了解，但也没有关系。 数据怎么样才能对人有用呢？其实数据本身不是有用的，必须要经过一定的处理。例如你每天跑步带个手环收集的也是数据，网上这么多网页也是数据，我们称为Data，数据本身没有什么用处，但是数据里面包含一个很重要的东西，叫做信息Information，数据十分杂乱，经过梳理和清洗，才能够称为信息。信息会包含很多规律，我们需要从信息中将规律总结出来，称为知识knowledge，知识改变命运。信息是很多的，但是有人看到了信息相当于白看，但是有人就从信息中看到了电商的未来，有人看到了直播的未来，所以人家就牛了，你如果没有从信息中提取出知识，天天看朋友圈，也只能在互联网滚滚大潮中做个看客。有了知识，然后利用这些知识去应用于实战，有的人会做得非常好，这个东西叫做智慧intelligence。有知识并不一定有智慧，例如好多学者很有知识，已经发生的事情可以从各个角度分析的头头是道，但一到实干就歇菜，并不能转化成为智慧。而很多的创业家之所以伟大，就是通过获得的知识应用于实践，最后做了很大的生意。 所以数据的应用分这四个步骤：数据，信息，知识，智慧。这是很多商家都想要的，你看我收集了这么多的数据，能不能基于这些数据来帮我做下一步的决策，改善我的产品，例如让用户看视频的时候旁边弹出广告，正好是他想买的东西，再如让用户听音乐的时候，另外推荐一些他非常想听的其他音乐。用户在我的应用或者网站上随便点点鼠标，输入文字对我来说都是数据，我就是要将其中某些东西提取出来，指导实践，形成智慧，让用户陷入到我的应用里面不可自拔，上了我的网就不想离开，手不停的点，不停的买，很多人说双十一我都想断网了，我老婆在上面不断的买买买，买了A又推荐B，老婆大人说，“哎呀，B也是我喜欢的啊，老公我要买”。你说这个程序怎么这么牛，这么有智慧，比我还了解我老婆，这件事情是怎么做到的呢？ 3.2 数据如何升华为智慧 数据的处理分几个步骤，完成了才最后会有智慧。 第一个步骤叫数据的收集。首先得有数据，数据的收集有两个方式，第一个方式是拿，专业点的说法叫抓取或者爬取，例如搜索引擎就是这么做的，它把网上的所有的信息都下载到它的数据中心，然后你一搜才能搜出来。比如你去搜索的时候，结果会是一个列表，这个列表为什么会在搜索引擎的公司里面呢，就是因为他把这个数据啊都拿下来了，但是你一点链接，点出来这个网站就不在搜索引擎它们公司了。比如说新浪有个新闻，你拿百度搜出来，你不点的时候，那一页在百度数据中心，一点出来的网页就是在新浪的数据中心了。另外一个方式就是推送，有很多终端可以帮我收集数据，比如说小米手环，可以将你每天跑步的数据，心跳的数据，睡眠的数据都上传到数据中心里面。 第二个步骤是数据的传输。一般会通过队列方式进行，因为数据量实在是太大了，数据必须经过处理才会有用，可是系统处理不过来，只好排好队，慢慢的处理。 第三个步骤是数据的存储。现在数据就是金钱，掌握了数据就相当于掌握了钱。要不然网站怎么知道你想买什么呢？就是因为它有你历史的交易的数据，这个信息可不能给别人，十分宝贵，所以需要存储下来。 第四个步骤是数据的处理和分析。上面存储的数据是原始数据，原始数据多是杂乱无章的，有很多垃圾数据在里面，因而需要清洗和过滤，得到一些高质量的数据。对于高质量的数据，就可以进行分析，从而对数据进行分类，或者发现数据之间的相互关系，得到知识。比如盛传的沃尔玛超市的啤酒和尿布的故事，就是通过对人们的购买数据进行分析，发现了男人一般买尿布的时候，会同时购买啤酒，这样就发现了啤酒和尿布之间的相互关系，获得知识，然后应用到实践中，将啤酒和尿布的柜台弄的很近，就获得了智慧。 第五个步骤就是对于数据的检索和挖掘。检索就是搜索，所谓外事不决问google，内事不决问百度。内外两大搜索引擎都是讲分析后的数据放入搜索引擎，从而人们想寻找信息的时候，一搜就有了。另外就是挖掘，仅仅搜索出来已经不能满足人们的要求了，还需要从信息中挖掘出相互的关系。比如财经搜索，当搜索某个公司股票的时候，该公司的高管是不是也应该被挖掘出来呢？如果仅仅搜索出这个公司的股票发现涨的特别好，于是你就去买了，其实其高管发了一个声明，对股票十分不利，第二天就跌了，这不坑害广大股民么？所以通过各种算法挖掘数据中的关系，形成知识库，十分重要。 3.3 大数据时代，众人拾柴火焰高 当数据量很小的时候，很少的几台机器就能解决。慢慢的当数据量越来越大，最牛的服务器都解决不了问题的时候，就想怎么办呢？要聚合多台机器的力量，大家齐心协力一起把这个事搞定，众人拾柴火焰高。 对于数据的收集，对于IoT来讲，外面部署这成千上万的检测设备，将大量的温度，适度，监控，电力等等数据统统收集上来，对于互联网网页的搜索引擎来讲，需要将整个互联网所有的网页都下载下来，这显然一台机器做不到，需要多台机器组成网络爬虫系统，每台机器下载一部分，同时工作，才能在有限的时间内，将海量的网页下载完毕。 对于数据的传输，一个内存里面的队列肯定会被大量的数据挤爆掉，于是就产生了基于硬盘的分布式队列，这样队列可以多台机器同时传输，随你数据量多大，只要我的队列足够多，管道足够粗，就能够撑得住。 对于数据的存储，一台机器的文件系统肯定是放不下了，所以需要一个很大的分布式文件系统来做这件事情，把多台机器的硬盘打成一块大的文件系统。 再如数据的分析，可能需要对大量的数据做分解，统计，汇总，一台机器肯定搞不定，处理到猴年马月也分析不完，于是就有分布式计算的方法，将大量的数据分成小份，每台机器处理一小份，多台机器并行处理，很快就能算完。例如著名的Terasort对1个TB的数据排序，相当于1024G，如果单机处理，怎么也要几个小时，但是并行处理209秒就完成了。 所以说大数据平台，什么叫做大数据，说白了就是一台机器干不完，大家一起干。随着数据量越来越大，很多不大的公司都需要处理相当多的数据，这些小公司没有这么多机器可怎么办呢？ 3.4 大数据需要云计算，云计算需要大数据 说到这里，大家想起云计算了吧。当想要干这些活的时候，需要好多好多的机器一块做，真的是想什么时候要，想要多少就要多少。例如大数据分析公司的财务情况，可能一周分析一次，如果要把这一百台机器或者一千台机器都在那放着，一周用一次对吧，非常浪费。那能不能需要计算的时候，把这一千台机器拿出来，然后不算的时候，这一千台机器可以去干别的事情。谁能做这个事儿呢？只有云计算，可以为大数据的运算提供资源层的灵活性。而云计算也会部署大数据放到它的PaaS平台上，作为一个非常非常重要的通用应用。因为大数据平台能够使得多台机器一起干一个事儿，这个东西不是一般人能开发出来的，也不是一般人玩得转的，怎么也得雇个几十上百号人才能把这个玩起来，所以说就像数据库一样，其实还是需要有一帮专业的人来玩这个东西。现在公有云上基本上都会有大数据的解决方案了，一个小公司我需要大数据平台的时候，不需要采购一千台机器，只要到公有云上一点，这一千台机器都出来了，并且上面已经部署好了的大数据平台，只要把数据放进去算就可以了。 云计算需要大数据，大数据需要云计算，两个人就这样结合了。 四、人工智能拥抱大数据4.1 机器什么时候才能懂人心 虽说有了大数据，人的欲望总是这个不能够满足。虽说在大数据平台里面有搜索引擎这个东西，想要什么东西我一搜就出来了。但是也存在这样的情况，我想要的东西不会搜，表达不出来，搜索出来的又不是我想要的。例如音乐软件里面推荐一首歌，这首歌我没听过，当然不知道名字，也没法搜，但是软件推荐给我，我的确喜欢，这就是搜索做不到的事情。当人们使用这种应用的时候，会发现机器知道我想要什么，而不是说当我想要的时候，去机器里面搜索。这个机器真像我的朋友一样懂我，这就有点人工智能的意思了。 人们很早就在想这个事情了。最早的时候，人们想象，如果要是有一堵墙，墙后面是个机器，我给它说话，它就给我回应，我如果感觉不出它那边是人还是机器，那它就真的是一个人工智能的东西了。 4.2 让机器学会推理 怎么才能做到这一点呢？人们就想：我首先要告诉计算机人类的推理的能力。你看人重要的是什么呀，人和动物的区别在什么呀，就是能推理。我要是把我这个推理的能力啊告诉机器，机器就能根据你的提问，推理出相应的回答，真能这样多好。推理其实人们慢慢的让机器能够做到一些了，例如证明数学公式。这是一个非常让人惊喜的一个过程，机器竟然能够证明数学公式。但是慢慢发现其实这个结果，也没有那么令人惊喜，因为大家发现了一个问题，数学公式非常严谨，推理过程也非常严谨，而且数学公式很容易拿机器来进行表达，程序也相对容易表达。然而人类的语言就没这么简单了，比如今天晚上，你和你女朋友约会，你女朋友说：如果你早来，我没来，你等着，如果我早来，你没来，你等着。这个机器就比比较难理解了，但是人都懂，所以你和女朋友约会，你是不敢迟到的。 4.3 教给机器知识 所以仅仅告诉机器严格的推理是不够的，还要告诉机器一些知识。但是知识这个事儿，一般人可能就做不来了，可能专家可以，比如语言领域的专家，或者财经领域的专家。语言领域和财经领域知识能不能表示成像数学公式一样稍微严格点呢？例如语言专家可能会总结出主谓宾定状补这些语法规则，主语后面一定是谓语，谓语后面一定是宾语，将这些总结出来，并严格表达出来不久行了吗？后来发现这个不行，太难总结了，语言表达千变万化。就拿主谓宾的例子，很多时候在口语里面就省略了谓语，别人问：你谁啊？我回答：我刘超。但是你不能规定在语音语义识别的时候，要求对着机器说标准的书面语，这样还是不够智能，就像罗永浩在一次演讲中说的那样，每次对着手机，用书面语说：请帮我呼叫某某某，这是一件很尴尬的事情。 人工智能这个阶段叫做专家系统。专家系统不易成功，一方面是知识比较难总结，另一方面总结出来的知识难以教给计算机。因为你自己还迷迷糊糊，似乎觉得有规律，就是说不出来，就怎么能够通过编程教给计算机呢？ 4.4 算了，教不会你自己学吧 于是人们想到，看来机器是和人完全不一样的物种，干脆让机器自己学习好了。机器怎么学习呢？既然机器的统计能力这么强，基于统计学习，一定能从大量的数字中发现一定的规律。 其实在娱乐圈有很好的一个例子，可见一斑 有一位网友统计了知名歌手在大陆发行的 9 张专辑中 117 首歌曲的歌词，同一词语在一首歌出现只算一次，形容词、名词和动词的前十名如下表所示（词语后面的数字是出现的次数）： a 形容词 b 名词 c 动词 0 孤独:34 0 生命:50 0 爱:54 1 自由:17 1 路:37 1 碎:37 2 迷惘:16 2 夜:29 2 哭:35 3 坚强:13 3 天空:24 3 死:27 4 绝望:8 4 孩子:23 4 飞:26 5 青春:7 5 雨:21 5 梦想:14 6 迷茫:6 6 石头:9 6 祈祷:10 7 光明:6 7 鸟:9 7 离去:10 如果我们随便写一串数字，然后按照数位依次在形容词、名词和动词中取出一个词，连在一起会怎么样呢？ 例如取圆周率 3.1415926，对应的词语是：坚强，路，飞，自由，雨，埋，迷惘。稍微连接和润色一下： 坚强的孩子， 依然前行在路上， 张开翅膀飞向自由， 让雨水埋葬他的迷惘。 是不是有点感觉了？当然真正基于统计的学习算法比这个简单的统计复杂的多。 然而统计学习比较容易理解简单的相关性，例如一个词和另一个词总是一起出现，两个词应该有关系，而无法表达复杂的相关性，并且统计方法的公式往往非常复杂，为了简化计算，常常做出各种独立性的假设，来降低公式的计算难度，然而现实生活中，具有独立性的事件是相对较少的。 4.5 模拟大脑的工作方式 于是人类开始从机器的世界，反思人类的世界是怎么工作的。 人类的脑子里面不是存储着大量的规则，也不是记录着大量的统计数据，而是通过神经元的触发实现的，每个神经元有从其他神经元的输入，当接收到输入的时候，会产生一个输出来刺激其他的神经元，于是大量的神经元相互反应，最终形成各种输出的结果。例如当人们看到美女瞳孔放大，绝不是大脑根据身材比例进行规则判断，也不是将人生中看过的所有的美女都统计一遍，而是神经元从视网膜触发到大脑再回到瞳孔。在这个过程中，其实很难总结出每个神经元对最终的结果起到了哪些作用，反正就是起作用了。 于是人们开始用一个数学单元模拟神经元 这个神经元有输入，有输出，输入和输出之间通过一个公式来表示，输入根据重要程度不同(权重)，影响着输出。 于是将n个神经元通过像一张神经网络一样连接在一起，n这个数字可以很大很大，所有的神经元可以分成很多列，每一列很多个排列起来，每个神经元的对于输入的权重可以都不相同，从而每个神经元的公式也不相同。当人们从这张网络中输入一个东西的时候，希望输出一个对人类来讲正确的结果。例如上面的例子，输入一个写着2的图片，输出的列表里面第二个数字最大，其实从机器来讲，它既不知道输入的这个图片写的是2，也不知道输出的这一系列数字的意义，没关系，人知道意义就可以了。正如对于神经元来说，他们既不知道视网膜看到的是美女，也不知道瞳孔放大是为了看的清楚，反正看到美女，瞳孔放大了，就可以了。 对于任何一张神经网络，谁也不敢保证输入是2，输出一定是第二个数字最大，要保证这个结果，需要训练和学习。毕竟看到美女而瞳孔放大也是人类很多年进化的结果。学习的过程就是，输入大量的图片，如果结果不是想要的结果，则进行调整。如何调整呢，就是每个神经元的每个权重都向目标进行微调，由于神经元和权重实在是太多了，所以整张网络产生的结果很难表现出非此即彼的结果，而是向着结果微微的进步，最终能够达到目标结果。当然这些调整的策略还是非常有技巧的，需要算法的高手来仔细的调整。正如人类见到美女，瞳孔一开始没有放大到能看清楚，于是美女跟别人跑了，下次学习的结果是瞳孔放大一点点，而不是放大鼻孔。 4.6 没道理但做得到 听起来也没有那么有道理，但是的确能做到，就是这么任性。 神经网络的普遍性定理是这样说的，假设某个人给你某种复杂奇特的函数，f(x)： 不管这个函数是什么样的，总会确保有个神经网络能够对任何可能的输入x，其值f(x)（或者某个能够准确的近似）是神经网络的输出。 如果在函数代表着规律，也意味着这个规律无论多么奇妙，多么不能理解，都是能通过大量的神经元，通过大量权重的调整，表示出来的。 4.7 人工智能的经济学解释 这让我想到了经济学，于是比较容易理解了。 我们把每个神经元当成社会中从事经济活动的个体。于是神经网络相当于整个经济社会，每个神经元对于社会的输入，都有权重的调整，做出相应的输出，比如工资涨了，菜价也涨了，股票跌了，我应该怎么办，怎么花自己的钱。这里面没有规律么？肯定有，但是具体什么规律呢？却很难说清楚。 基于专家系统的经济属于计划经济，整个经济规律的表示不希望通过每个经济个体的独立决策表现出来，而是希望通过专家的高屋建瓴和远见卓识总结出来。专家永远不可能知道哪个城市的哪个街道缺少一个卖甜豆腐脑的。于是专家说应该产多少钢铁，产多少馒头，往往距离人民生活的真正需求有较大的差距，就算整个计划书写个几百页，也无法表达隐藏在人民生活中的小规律。 基于统计的宏观调控就靠谱的多了，每年统计局都会统计整个社会的就业率，通胀率，GDP等等指标，这些指标往往代表着很多的内在规律，虽然不能够精确表达，但是相对靠谱。然而基于统计的规律总结表达相对比较粗糙，比如经济学家看到这些统计数据可以总结出长期来看房价是涨还是跌，股票长期来看是涨还是跌，如果经济总体上扬，房价和股票应该都是涨的。但是基于统计数据，无法总结出股票，物价的微小波动规律。 基于神经网络的微观经济学才是对整个经济规律最最准确的表达，每个人对于从社会中的输入，进行各自的调整，并且调整同样会作为输入反馈到社会中。想象一下股市行情细微的波动曲线，正是每个独立的个体各自不断交易的结果，没有统一的规律可循。而每个人根据整个社会的输入进行独立决策，当某些因素经过多次训练，也会形成宏观上的统计性的规律，这也就是宏观经济学所能看到的。例如每次货币大量发行，最后房价都会上涨，多次训练后，人们也就都学会了。 4.8 人工智能需要大数据 然而神经网络包含这么多的节点，每个节点包含非常多的参数，整个参数量实在是太大了，需要的计算量实在太大，但是没有关系啊，我们有大数据平台，可以汇聚多台机器的力量一起来计算，才能在有限的时间内得到想要的结果。 人工智能可以做的事情非常多，例如可以鉴别垃圾邮件，鉴别黄色暴力文字和图片等。这也是经历了三个阶段的。第一个阶段依赖于关键词黑白名单和过滤技术，包含哪些词就是黄色或者暴力的文字。随着这个网络语言越来越多，词也不断的变化，不断的更新这个词库就有点顾不过来。第二个阶段时，基于一些新的算法，比如说贝叶斯过滤等，你不用管贝叶斯算法是什么，但是这个名字你应该听过，这个一个基于概率的算法。第三个阶段就是基于大数据和人工智能，进行更加精准的用户画像和文本理解和图像理解。 由于人工智能算法多是依赖于大量的数据的，这些数据往往需要面向某个特定的领域(例如电商，邮箱)进行长期的积累，如果没有数据，就算有人工智能算法也白搭，所以人工智能程序很少像前面的IaaS和PaaS一样，将人工智能程序给某个客户安装一套让客户去用，因为给某个客户单独安装一套，客户没有相关的数据做训练，结果往往是很差的。但是云计算厂商往往是积累了大量数据的，于是就在云计算厂商里面安装一套，暴露一个服务接口，比如您想鉴别一个文本是不是涉及黄色和暴力，直接用这个在线服务就可以了。这种形势的服务，在云计算里面称为软件即服务，SaaS (Software AS A Service) 于是工智能程序作为SaaS平台进入了云计算。 五、云计算，大数据，人工智能过上了美好的生活 终于云计算的三兄弟凑齐了，分别是IaaS，PaaS和SaaS，所以一般在一个云计算平台上，云，大数据，人工智能都能找得到。对一个大数据公司，积累了大量的数据，也会使用一些人工智能的算法提供一些服务。对于一个人工智能公司，也不可能没有大数据平台支撑。所以云计算，大数据，人工智能就这样整合起来，完成了相遇，相识，相知。","categories":[],"tags":[{"name":"后端","slug":"后端","permalink":"https://xixfeng.github.io/tags/后端/"}],"keywords":[]},{"title":"SpringBoot知识","slug":"SpringBoot֪ʶ","date":"2018-11-14T09:54:19.000Z","updated":"2019-06-12T08:11:22.731Z","comments":true,"path":"2018/11/14/SpringBoot֪ʶ/","link":"","permalink":"https://xixfeng.github.io/2018/11/14/SpringBoot֪ʶ/","excerpt":"","text":"@RequestBody 注入http请求的请求体 //post时，为请求表单 data参数是字符串类型。 type必须是post类型，get类型会报错 必须加上请求内容类型为json 类型 contentType : “application/json;charset=utf-8”, application/x-www-form-urlencoded， 可选（即非必须，因为这种情况的数据@RequestParam（？？？？？可以吗）, @ModelAttribute也可以处理，当然@RequestBody也能处理）； ​ multipart/form-data, 不能处理（即使用@RequestBody不能处理这种格式的数据）； ​ 其他格式， 必须（其他格式包括application/json, application/xml等。这些格式的数据，必须使用@RequestBody来处理）； @RequestBody最好用来处理json格式 使用ajax给后端传图片时，不用加content-type &lt;input type=&quot;file&quot; ref=&quot;file&quot;/&gt; &lt;span @click=&quot;getFile &quot;&gt;提交&lt;/span&gt; getFile () { console.log(this.$refs.file.files) } 例2： &lt;input type=&quot;file&quot; @change=&quot;getFile &quot;/&gt; getFile (e) { console.log(e.target.files) } @RequestParam 取body里面的参数（不管是POST的表单还是GET的query都是放在body中的 FormData对象用以将数据编译成键值对，以便用XMLHttpRequest来发送数据。其主要用于发送表单数据，但亦可用于发送带键数据(keyed data)，而独立于表单使用。如果表单enctype属性设为multipart/form-data ，则会使用表单的submit()方法来发送数据，从而，发送数据具有同样形式。不用去用qs去解析 md axios中当使用FormData时，会自动 Content-type：multipart/form-data;？ SpringBoot静态资源更新后，需要重启才能访问到 7.在SpringBoot中我们用bean类与数据库对应，当bean类的变量名称与数据库不一致时，可以用@Results建立映射 当使用@Results(也可以从英文上理解)对应，并且只返回这些数据 @Results({ @Result(property = &quot;id&quot;, column = &quot;id&quot;), @Result(property = &quot;pageId&quot;, column = &quot;page_id&quot;), @Result(property = &quot;studentId&quot;, column = &quot;student_id&quot;), @Result(property = &quot;content&quot;,column = &quot;content&quot;), @Result(property = &quot;time&quot;,column = &quot;time&quot;), @Result(property = &quot;like&quot;,column = &quot;like&quot;), @Result(property = &quot;unlike&quot;,column = &quot;unlike&quot;), @Result(property = &quot;score&quot;,column = &quot;score&quot;), @Result(property = &quot;pic&quot;,column = &quot;pic&quot;), @Result(property = &quot;page&quot;,column = &quot;page_id&quot;,one=@One(select=&quot;com.scuknow.mapper.PageMapper.findPageById&quot;)), @Result(property = &quot;user&quot;,column = &quot;student_id&quot;, one=@One(select=&quot;com.scuknow.mapper.UserMapper.findUserBasicInfoById&quot;)) }) 8.Mysql分页查询 9.SpringBoot查询，使用其他mapper的查询 @Result(property = &quot;user&quot;, column = &quot;student_id&quot;, one = @One(select = &quot;com.scuknow.mapper.UserMapper.findUserBasicInfoById&quot;)) 10.使用bean类去映射数据库表时，一定要设置好bean的getter setter，不然不会返回数据 用bean类去映射，或者注入sql语句时，给参数 bean映射一般用于需要返回数据时 11. 第一个参数为bean类里面的，第二个参数为数据表中的，第三个参数通过数据表中的第二个参数查找 @Result(property = &quot;followList&quot;, column = &quot;fstudent_id&quot;, many = @Many(select = &quot;com.scuknow.mapper.UserMapper.findUserBasicInfoById&quot;)) 12.注意数据库的int和bigint 13. 进阶操作——组合索引的唯一性前面只讲了给一个字段设置唯一性的方法，如果有两个或多个字段的组合需要唯一呢？例子：有一个保存大四毕业生每周上传的毕设报告的表。它有学生id：sid，周数：week，报告内容：content等字段。这里只允许每个学生每周上传一份周报告，即sid和week的组合不能出现重复的情况，也就是说不能有两条记录他们的sid和week都相同。如何在mysql中进行设置呢，这里只给出navicat中的方法：选中navicat设计表中的引索 #","categories":[],"tags":[{"name":"后端","slug":"后端","permalink":"https://xixfeng.github.io/tags/后端/"}],"keywords":[]},{"title":"微信小程序简单学习","slug":"微信小程序简单学习","date":"2018-11-11T10:04:52.000Z","updated":"2019-06-12T08:11:15.133Z","comments":true,"path":"2018/11/11/微信小程序简单学习/","link":"","permalink":"https://xixfeng.github.io/2018/11/11/微信小程序简单学习/","excerpt":"","text":"和vue不一样，微信小程序获取页面data是this.data input标签，数据交换，通过bindinput绑定一个监控函数，当input框内值改变时，便会调用这个函数 &lt;input type=&#39;text&#39; bindinput=&quot;getImageName&quot; placeholder=&quot;Image name&quot; auto-focus/&gt; getImageName: function (e) { this.setData({ newimagename: e.detail.value }) console.log(this.data.newimagename)//.data } 3.把标签的属性值传送给js，通过绑定data-name属性传给js，注意name不能大写 &lt;wux-button block type=&quot;positive&quot; bindtap=&quot;delete&quot; data-id=&quot;{{item.Id}}&quot;&gt;删除镜像&lt;/wux-button&gt; delete:function(e){ var that = this; var tem = &#39;http://www.xixfeng.cn:8999/dockerapi/images/&#39;+e.currentTarget.dataset.id wx.request({ url: tem, data: { }, header: { &#39;content-type&#39;: &#39;application/json, text/plain, */*&#39;, // 默认值 }, method:&quot;DELETE&quot;, success(res) { that.getInfo() wx.hideToast() console.log(res.data); } }) }","categories":[],"tags":[{"name":"前端","slug":"前端","permalink":"https://xixfeng.github.io/tags/前端/"}],"keywords":[]},{"title":"排序","slug":"排序","date":"2018-11-10T12:58:16.000Z","updated":"2019-06-12T08:11:09.830Z","comments":true,"path":"2018/11/10/排序/","link":"","permalink":"https://xixfeng.github.io/2018/11/10/排序/","excerpt":"","text":"一些想法可以通过 递归树来判断 递归函数的复杂度 排序类public class Sort { public static void main(String[] args) {//测试 Integer[] test = {8,7,4,5,111,43,22}; long startTime = System.nanoTime(); //获取开始时间 Sort.sort(test); long endTime = System.nanoTime(); //获取结束时间 System.out.println((endTime-startTime)+&quot;ns&quot;); Sort.show(test); } public static void exchange(Comparable[] a,int i,int j){ Comparable t = a[i]; a[i] = a[j]; a[j] = t; } public static void show(Comparable[] a){ for(int i=0;i&lt;a.length;i++){ System.out.println(a[i]); } } public static void sort(Comparable[] a){ } } 选择排序 public static void xsort(Comparable[] a){ int N = a.length; for(int i=0;i&lt;N;i++){ for(int j=i+1;j&lt;N;j++){ if(a[i].compareTo(a[j])&gt;0)exchange(a, i, j); } } } 插入排序 public static void csort(Comparable[] a){ for(int i=0;i&lt;a.length;i++){//光标i之前元素都有序 所以移动i到之前合适位置就可以了 for(int j=i-1;j&gt;=0;j--){ if(a[i].compareTo(a[j])&lt;0)exchange(a, i, j); else break; } } } 希尔排序 public static void sort(Comparable[] a){ int n = 1; int N = a.length; while(n&lt;N/3)n = n*3+1;//找到n可取的最大值 但是为什么要这样呢？ while(n&gt;0){//每次循环都将数组变成 n有序数组 for(int i=n;i&lt;N;i++){//结合插入排序 for(int j=i-n;j&gt;=0;j-=n){ if(a[i].compareTo(a[j])&lt;0)exchange(a, i, j); else break; } } n = n/3; } } 归并排序 private static void merge(Comparable[] a,int lo,int mid,int hi){ int i = lo; int j = mid+1; Comparable[] aux = new Comparable[a.length]; for(int k=lo;k&lt;=hi;k++)aux[k] = a[k];//这是什么操作 for(int k=lo;k&lt;=hi;k++){ if(i&gt;mid) a[k] = aux[j++]; else if(j&gt;hi) a[k] = aux[i++]; else if(aux[i].compareTo(aux[j])&lt;0) a[k] = aux[i++]; else a[k] = aux[j++]; } } //自顶向下排序 public static void sort(Comparable[] a,int lo,int hi){ if(lo&gt;=hi)return; int mid = lo+(hi-lo)/2; sort(a,lo,mid);//将左半边排序 sort(a,mid+1,hi);//将右半边排序 merge(a, lo, mid, hi);//将两边结果合并 } 一些问题考虑这样一个递归的归并排序，当数组小于某个阀值时，使用插入排序，如果归并排序调用了n次，那么插入排序会被调用几次？为什么？ n次；一次归并代表一个 递归树上的节点，则 递归树高度为log2(n)，我们会在 递归树最后一层调用插入排序，最后一层的节点数目为n。 快速排序快速排序的最坏情况，当选取的指标为最小值/最大值时，递归树深度为n。 快速排序的最好情况，当选取的指标为中间值时，递归树深度为对数。 private static int partition(Comparable[] a,int lo,int hi){ Comparable t = a[hi]; int i = lo-1; int j = hi; while(true){ while(a[++i].compareTo(t)&lt;0)if(i==hi)break; while(a[--j].compareTo(t)&gt;0)if(j==lo)break; if(i&gt;=j)break; exchange(a, i, j); } exchange(a, hi, i); return i; } public static void sort(Comparable[] a,int lo,int hi){ if(lo&gt;=hi)return; int mid = partition(a, lo, hi); sort(a, lo, mid-1); sort(a, mid+1, hi); }","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://xixfeng.github.io/tags/算法/"}],"keywords":[]},{"title":"java实现抽象数据结构","slug":"java实现抽象数据结构","date":"2018-11-08T16:05:27.000Z","updated":"2019-06-13T15:34:40.791Z","comments":true,"path":"2018/11/09/java实现抽象数据结构/","link":"","permalink":"https://xixfeng.github.io/2018/11/09/java实现抽象数据结构/","excerpt":"","text":"双链表实现class DoubleLink&lt;T&gt;{ private Node&lt;T&gt; hNode; private int mCount; class Node&lt;T&gt;{ private Node prev; private Node next; private T value; public Node(T value,Node prev,Node next){ this.prev = prev; this.next = next; this.value = value; } } public DoubleLink(){ hNode = new Node&lt;T&gt;(null, null, null); mCount = 0; } public int size(){ return mCount; } public boolean isEmpty(){ return mCount == 0; } private Node&lt;T&gt; getNode(int index){ if(index&lt;0||index&gt;=mCount)throw new IndexOutOfBoundsException();//******/ if(index&lt;=mCount/2){ Node iNode = hNode.next; for(int i=0;i&lt;index;i++){ iNode = iNode.next; } return iNode; } else{ int rindex = mCount-index-1; Node rNode = hNode.prev; for(int i=0;i&lt;rindex;i++){ rNode = rNode.prev; } return rNode; } } public T get(int index){ return getNode(index).value; } public T getFirst(){ return getNode(0).value; } public T getLast(){ return getNode(mCount-1).value; } //插入到index节点之前 public void insert(int index,T value){ if (mCount==0) { Node&lt;T&gt; node = new Node&lt;T&gt;(value, hNode, hNode); hNode.next = node; hNode.prev = node; mCount++; return ; } if(index==mCount-1){ //Node&lt;T&gt; tnode = getNode(index); 这一步可以省略 Node&lt;T&gt; node = new Node&lt;T&gt;(value,hNode.prev,hNode); hNode.prev.next = node; hNode.prev = node; mCount++; return; } Node&lt;T&gt; node = getNode(index); Node&lt;T&gt; tNode = new Node&lt;T&gt;(value,node,node.next); node.next.prev = tNode; node.next = tNode; mCount++; return; } public void insertFirst(T value){ Node&lt;T&gt; node = new Node&lt;T&gt;(value,hNode,hNode.next); hNode.next.prev = node; hNode.next = node; mCount++; } public void appendLast(T value){ insert(mCount-1, value); } public void delete(int index){ Node&lt;T&gt; node = getNode(index); node.prev.next = node.next; node.next.prev = node.prev; node = null;//java的辣鸡回收机制，会回收无法访问的内存 mCount--; } public void deleteFirst(){ delete(0); } public void deleteLast(){ delete(mCount-1); } } 测试public class DlinkTest { // 双向链表操作int数据 private static void int_test() { System.out.println(&quot;\\n----int_test----&quot;); // 创建双向链表 DoubleLink&lt;Integer&gt; dlink = new DoubleLink&lt;Integer&gt;(); dlink.insert(0, 20); // 将 20 插入到第一个位置 dlink.appendLast(10); // 将 10 追加到链表末尾 dlink.insertFirst(30); // 将 30 插入到第一个位置 // 双向链表是否为空 System.out.printf(&quot;isEmpty()=%b\\n&quot;, dlink.isEmpty()); // 双向链表的大小 System.out.printf(&quot;size()=%d\\n&quot;, dlink.size()); // 打印出全部的节点 for (int i=0; i&lt;dlink.size(); i++) System.out.println(&quot;dlink(&quot;+i+&quot;)=&quot;+ dlink.get(i)); } private static void string_test() { String[] sarr = {&quot;ten&quot;, &quot;twenty&quot;, &quot;thirty&quot;, &quot;forty&quot;}; System.out.println(&quot;\\n----string_test----&quot;); // 创建双向链表 DoubleLink&lt;String&gt; dlink = new DoubleLink&lt;String&gt;(); dlink.insert(0, sarr[1]); // 将 sarr中第2个元素 插入到第一个位置 dlink.appendLast(sarr[0]); // 将 sarr中第1个元素 追加到链表末尾 dlink.insertFirst(sarr[2]); // 将 sarr中第3个元素 插入到第一个位置 // 双向链表是否为空 System.out.printf(&quot;isEmpty()=%b\\n&quot;, dlink.isEmpty()); // 双向链表的大小 System.out.printf(&quot;size()=%d\\n&quot;, dlink.size()); // 打印出全部的节点 for (int i=0; i&lt;dlink.size(); i++) System.out.println(&quot;dlink(&quot;+i+&quot;)=&quot;+ dlink.get(i)); } // 内部类 private static class Student { private int id; private String name; public Student(int id, String name) { this.id = id; this.name = name; } @Override public String toString() { return &quot;[&quot;+id+&quot;, &quot;+name+&quot;]&quot;; } } private static Student[] students = new Student[]{ new Student(10, &quot;sky&quot;), new Student(20, &quot;jody&quot;), new Student(30, &quot;vic&quot;), new Student(40, &quot;dan&quot;), }; private static void object_test() { System.out.println(&quot;\\n----object_test----&quot;); // 创建双向链表 DoubleLink&lt;Student&gt; dlink = new DoubleLink&lt;Student&gt;(); dlink.insert(0, students[1]); // 将 students中第2个元素 插入到第一个位置 dlink.appendLast(students[0]); // 将 students中第1个元素 追加到链表末尾 dlink.insertFirst(students[2]); // 将 students中第3个元素 插入到第一个位置 // 双向链表是否为空 System.out.printf(&quot;isEmpty()=%b\\n&quot;, dlink.isEmpty()); // 双向链表的大小 System.out.printf(&quot;size()=%d\\n&quot;, dlink.size()); // 打印出全部的节点 for (int i=0; i&lt;dlink.size(); i++) { System.out.println(&quot;dlink(&quot;+i+&quot;)=&quot;+ dlink.get(i)); } } public static void main(String[] args) { int_test(); // 演示向双向链表操作“int数据”。 string_test(); // 演示向双向链表操作“字符串数据”。 object_test(); // 演示向双向链表操作“对象”。 } } 栈java util包里面有Stack类 主要方法 push(向栈中添加元素) peek(返回栈顶元素) pop(返回并删除栈顶元素) import java.lang.reflect.Array; public class GeneralArrayStack&lt;T&gt; { private static final int DEFAULT_SIZE = 12; private T[] mArray; private int count; public GeneralArrayStack(Class&lt;T&gt; type) { this(type, DEFAULT_SIZE); }//Class&lt;T&gt; T的类型类，不能通过一般的反射手段得到 public GeneralArrayStack(Class&lt;T&gt; type, int size) { // 不能直接使用mArray = new T[DEFAULT_SIZE]; mArray = (T[]) Array.newInstance(type, size); count = 0; } // 将val添加到栈中 public void push(T val) { mArray[count++] = val; } // 返回“栈顶元素值” public T peek() { return mArray[count-1]; } // 返回“栈顶元素值”，并删除“栈顶元素” public T pop() { T ret = mArray[count-1]; count--; return ret; } // 返回“栈”的大小 public int size() { return count; } // 返回“栈”是否为空 public boolean isEmpty() { return size()==0; } // 打印“栈” public void PrintArrayStack() { if (isEmpty()) { System.out.printf(&quot;stack is Empty\\n&quot;); } System.out.printf(&quot;stack size()=%d\\n&quot;, size()); int i=size()-1; while (i&gt;=0) { System.out.println(mArray[i]); i--; } } } 队列主要方法 入队列(先进先出) 出队列 树一些基本定义 叶子：出度为0的节点 深度：根节点到当前节点的长度(即线段数) 高度：最深节点的深度+1 二叉树定义 所有节点出度小于等于2 性质 在二叉树的i层上至多有2的i次方个节点 高度为k(k&gt;=1)的二叉树上至多含有2的k次方-1个节点 二叉树的高度为$$[\\log_{2}n]+1$$n为节点数 任意二叉树 N0 = N2 +1 遍历深度优先遍历 NLR 根左右 LNR 左右根 RNL 右左根 广度优先遍历？？？？？？？？？？？？？？？ 1.满二叉树定义：所有节点出度为0或者2 2.完全二叉树定义： 除最后一层，出度都为2 且最后一层，节点从左往右排 实现： 通过数组实现：设当前节点存于 i处，则左孩子位于 2i+1，右孩子位于 2i+2 3.二叉搜索树定义：左孩子 &lt; 双亲 &lt; 右孩子 实现 public class BSTree&lt;T extends Comparable&lt;T&gt;&gt; { private BSTNode&lt;T&gt; rNode; class BSTNode&lt;T extends Comparable&lt;T&gt;&gt;{ T key; BSTNode&lt;T&gt; parent; BSTNode&lt;T&gt; left; BSTNode&lt;T&gt; right; public BSTNode(T key,BSTNode&lt;T&gt; parent,BSTNode&lt;T&gt; left,BSTNode&lt;T&gt; right){ this.key = key; this.parent = parent; this.left = left; this.right = right; } } //搜索二叉树，一定是插入到叶节点中 private void insert(T key,BSTNode&lt;T&gt; node){ BSTNode&lt;T&gt; tNode = rNode; BSTNode&lt;T&gt; tNode2 = null; int tem = 1; while(tNode!=null){ tNode2 = tNode; tem = key.compareTo(tNode.key); if(tem==0)return; else if(tem&gt;0)tNode = tNode.right; else if(tem&lt;0)tNode = tNode.left; } node.parent = tNode2; if(tNode2==null)rNode = node;//插入时，如果搜索树为空 else{ if(tem&gt;0)tNode2.right = node; else tNode2.left = node; } } public void insert(T key){ BSTNode node = new BSTNode&lt;T&gt;(key, null, null, null); insert(key, node); } public void delete(T key){ //稍后补起 } private BSTNode&lt;T&gt; search(BSTNode&lt;T&gt; tNode,T key){ if(tNode==null)return null; int tem = key.compareTo(tNode.key); if(tem==0)return tNode; else if(tem&gt;0)tNode = tNode.right; else if(tem&lt;0)tNode = tNode.left; return search(tNode, key); } public BSTNode&lt;T&gt; search(T key){ return search(rNode, key); } public void printAll(BSTNode&lt;T&gt; node){ if (node==null)return; System.out.println(node.key); printAll(node.left); printAll(node.right); } public void printAll(){ printAll(rNode); } } 4.堆性质：1.是完全二叉树 2.任意节点值永远不小于（不大于）子节点 import java.util.ArrayList; class Main{ public static void main(String[] args) { Heap heap = new Heap&lt;Integer&gt;(); heap.insert(1); heap.insert(3); heap.insert(2); heap.insert(5); heap.insert(4); heap.insert(0); heap.delete(3); heap.printAll(); } } public class Heap&lt;T extends Comparable&lt;T&gt;&gt; { private ArrayList&lt;T&gt; heap; public Heap() { heap = new ArrayList&lt;T&gt;(); } public void printAll(){ for(int i=0;i&lt;heap.size();i++){ System.out.println(heap.get(i)); } } private void filterup(int start) { int c = start;// current int p = (c - 1) / 2; while (c &gt; 0) { if (heap.get(c).compareTo(heap.get(p)) &gt; 0) { exchange(c, p); c = p; p = (p - 1) / 2; } else break; } } public void insert(T t) { int size = heap.size(); heap.add(t);// 插在末尾 filterup(size);// 向上调整 } private void filterdown(int start,int end){ int c = start; int l = c*2+1; while(l&lt;=end){ int tem = heap.get(l).compareTo(heap.get(l+1));//这里l+1可能越界 if(l&lt;end&amp;&amp;tem&lt;0) l++; tem = heap.get(c).compareTo(heap.get(l)); if(tem&gt;0)break; else{ exchange(c, l); c = l; l = c*2+1; } } } public void delete(T t) { if (heap.isEmpty() == true) return; // 获取data在数组中的索引 int index = heap.indexOf(t); if (index == -1) return; int size = heap.size(); heap.set(index, heap.get(size - 1));// 用最后元素填补 heap.remove(size - 1); // 删除最后的元素 if (heap.size() &gt; 1)filterdown(index, heap.size() - 1); // 从index号位置开始自上向下调整为最小堆 } // public T search(T t){ // } // private void seftdown(int index){ // T parent = heap.get(index); // T left = heap.get(index*2+1); // T right = heap.get(index*2+2); // if(parent.compareTo(left)&lt;0||parent.compareTo(right)&lt;0){ // if(left.compareTo(right)&gt;0)exchange(index, index*2+1); // else exchange(index, index*2+2); // } // } private void exchange(int index, int index1) { T tem = heap.get(index); heap.set(index, heap.get(index1)); heap.set(index1, tem); } } 5.哈夫曼树一些定义 节点间的路径长度：连接两节点的路径上的分支数 节点的路径长度：根节点到该节点路径上的分支数 树加权路径长度：所有叶子节点加权路径长度之和 哈夫曼树（二叉树）：树加权路径长度最小的那种二叉树 通常用来解决编码问题","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://xixfeng.github.io/tags/数据结构/"}],"keywords":[]},{"title":"nginx","slug":"nginx","date":"2018-11-08T10:11:32.000Z","updated":"2019-06-13T15:34:20.730Z","comments":true,"path":"2018/11/08/nginx/","link":"","permalink":"https://xixfeng.github.io/2018/11/08/nginx/","excerpt":"","text":"nginx基本应用 http服务器。提供http服务，可以做网页静态服务器。 将自己已经注册的域名映射到静态文件夹 虚拟主机。可以实现在一台服务器虚拟出多个网站。例如个人网站使用的虚拟主机。 域名不同，端口相同。 域名相同，端口不同。 1和2是结合使用的 反向代理，负载均衡。当网站的访问量达到一定程度后，单台服务器不能满足用户的请求时，需要用多台服务器集群可以使用nginx做反向代理。并且多台服务器可以平均分担负载，不会因为某台服务器负载高宕机而某台服务器闲置的情况。 通过Keepalived实现微服务 nginx实现流服务器主要参考这篇文章 安装ffmpeg遇到的问题 安装nginx的openssl模块时，总会报错，原因：nginx版本不对 nginx添加模块，必须要用模块的源文件，也必须要用nginx当初编译安装时候的源文件 编译安装nginx并添加模块 sudo ./configure --prefix=/www/myserver/nginx --with-pcre=/www/myserver/nginx-dependence/pcre-8.40 --with-zlib=/www/myserver/nginx-dependence/zlib-1.2.11 --with-openssl=/www/myserver/nginx-dependence/openssl-1.1.0 --with-http_ssl_module --add-module=/www/myserver/nginx-dependence/nginx-rtmp-module worker_processes 1; events { worker_connections 1024; } rtmp { #RTMP服务 server { listen 1935; #//服务端口 chunk_size 4096; #//数据传输块的大小 application vod {#访问这个网址，nginx会自动跳到play这个位置 play /www/vide; #//视频文件存放位置。 } } } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } #源码 名词指令：配置文件里键值对的键就是指令，比如listen 数据结构1) struct ngx_command_s { ngx_str_t name; ngx_uint_t type; char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf); ngx_uint_t conf; ngx_uint_t offset; void *post; }; name：配置指令名称，如“proxy_pass”；type：指令类型，可以将指令类型分为两类，1）说明指令可以出现的位置，比如配置文件（只能在配置文件最外层，不能出现在任何指令块内部），http指令块，或者events指令块，或者server指令块，或者location指令块；2）用于校验参数数目set：处理函数，当读取到该配置指令时，会执行此函数； 2) typedef struct { ngx_int_t (*preconfiguration)(ngx_conf_t *cf); ngx_int_t (*postconfiguration)(ngx_conf_t *cf); void *(*create_main_conf)(ngx_conf_t *cf); char *(*init_main_conf)(ngx_conf_t *cf, void *conf); void *(*create_srv_conf)(ngx_conf_t *cf); char *(*merge_srv_conf)(ngx_conf_t *cf, void *prev, void *conf); void *(*create_loc_conf)(ngx_conf_t *cf); char *(*merge_loc_conf)(ngx_conf_t *cf, void *prev, void *conf); } ngx_http_module_t; 这个http_module和ngx_module_t有什么联系ngx_module_t-&gt;ctx会指向ngx_http_module_t3)http上下文 http_ctx`c // http_ctx typedef struct { void main_conf; void srv_conf; void **loc_conf;} ngx_http_conf_ctx_t; 4) ctx中的conf，一般是由我们自己创建的，模块感兴趣的参数结构体，比如说 typedef struct { ngx_array_t charsets; /* ngx_http_charset_t */ ngx_array_t tables; /* ngx_http_charset_tables_t */ ngx_array_t recodes; /* ngx_http_charset_recode_t */ } ngx_http_charset_main_conf_t; //要与整个配置文件的conf(下面👇）区分开 5) 指令属性结构体 存储每个指令的属性（上下文） struct ngx_conf_s { char *name; //当前读取到的指令名称 ngx_array_t *args; //当前读取到的指令参数 ngx_cycle_t *cycle; //指向全局cycle ngx_pool_t *pool; //内存池 ngx_conf_file_t *conf_file; //配置文件 void *ctx; //上下文 ngx_uint_t module_type; //模块类型 ngx_uint_t cmd_type; //指令类型 ngx_conf_handler_pt handler; //一般都是NULL，暂时不管 }; 6)nginx模块结构题 struct ngx_module_s { ngx_uint_t ctx_index; //用于给同类型的模块编号 ngx_uint_t index; //用于给所有模块编号 void *ctx; //模块上下文；很重要；不同类型的模块通常指向不同类型的结构体，结构体通常包含若干函数指针 ngx_command_t *commands; //指令数组 ngx_uint_t type; //模块类型编码 } type字段表示模块类型编码。ctx指向模块上下文结构体，且不同类型的模块通常指向不同类型的结构体，该结构体中通常会包含若干函数指针。 常见变量cmcf: ngx_http_core_module_t的ngx_http_core_main_conf_tcscf: ngx_http_core_srv_conf_tclcf: ngx_http_core_loc_conf_tcf: ngx_conf_tctx_index: 一个模块在全部http模块中的位置http_ctx: http的总ctx，三个指针指向三个数组ctx: 当前阶段的上下文conf-&gt;ctx: 配置文件的上下文一般指向模块 重要函数/* 遍历所有模块的command，找到与这个对应的指令对应的command 在解析配置文件时，遇到指令就会调用，比如http指令，server指令 args： cf：指令结构体 */ ngx_conf_handler(ngx_conf_t *cf, ngx_int_t last) 一些心得 可以在一个大系统里面定义好一些状态参数，以便处理返回结果 将配置存在上下文里 一些想法 nginx http模块整个的基本解析是交给core模块的，core模块去统筹所有其他http模块 我感觉更合理的做法应该是一开始只调用core的create_main然后逐步深入，而且一开始的时候也不用调用其他模块的create_srv和loc（这部分就暂定为无用吧） 配置项本身也是一种指令，只是它的参数有点特别 nginx配置文件存储 上下文ctx中包含配置结构conf, 1）为什么要将一个配置限定在一个上下文中呢？因为我们要确定一个配置结构体（配置项比如listen）属于哪个配置块 2）如何确定一个配置项在配置中的位置（在哪个server中，哪个location中）？根据上下文ctx的嵌套可以判断出配置项的位置，根据下面这种设计可以实现一个http嵌套server，server中嵌套location 3）但是上面那种无法实现http中嵌套多个server，不🉑️，所以需要将上面http_ctx中的srv_ctx改成srv_ctx数组，用数组下标表示是第几个上下文，下标为0代表http块中的第一个server块 4）但是为什么要做一个与main_conf同级的srv_conf和loc_conf呢？？？？？？ ##配置解析流程解析配置的入口函数是ngx_conf_parse(ngx_conf_t cf, ngx_str_t filename)，其输入参数filename表示配置文件路径，如果为NULL表明此时解析的是指令块。函数ngx_conf_parse逻辑比较简单，就是读取完整指令，并调用函数ngx_conf_handler处理指令。函数ngx_conf_handler主要逻辑是，遍历类型为cf-&gt;module_type的模块，查找该模块指令数组中类型为cf-&gt;cmd_type的指令；如果没找到打印错误日志并返回错误；如果找到还需要校验指令参数等是否合法；最后才是调用set函数设置。这些流程都比较简单，难点是如何根据ctx获取到该配置最终存储的位置。下面的代码需要结合上图来分析。配置肯定是存储在某个结构体的，所以需要通过ctx找到对应结构体。 ngx_http_core_module负责了所有conf的创建和存储，也就是说是由他的ctx存储的是http_conf //ngx_http_core_module（nginx实现的http模块）中ctx结构体的函数 /*什么也没做，只是生成了一个空的main_conf和srv_conf 最神奇的是他创建了一个存放srv_conf的数组？？？？ 所以可能很多的create_main create_srv都只是做了一些限定，而不是去取值*/ static void * ngx_http_core_create_main_conf(ngx_conf_t *cf) } //这个函数是core块解析遇到server块时调用的 /* 这个函数做的几件事情 1.调用所有模块http模块的create_srv,loc,并存储在ctx中 2.解析一个server块 */ static char * ngx_http_core_server(ngx_conf_t *cf, ngx_command_t *cmd, void *dummy) 一个周期的初始化ngx_init_cycle作用：解析nginx配置、初始化CORE模块，接着是初始化文件句柄，初始化错误日志，初始化共享内存，然后是监听端口；cycle的意思就是周期，而ngx_init_cycle就代表着一个周期的初始化 配置文件的解析在ngx_init_cycle中，配置文件的解析分为两个阶段：1.准备阶段 2.真正调用配置解析 准备阶段 1) 准备内存 nginx根据以往的经验（old_cycle）预测这一次的配置需要分配多少内存。比如，我们可以看这段： if (old_cycle-&gt;shared_memory.part.nelts) { n = old_cycle-&gt;shared_memory.part.nelts; for (part = old_cycle-&gt;shared_memory.part.next; part; part = part-&gt;next) { n += part-&gt;nelts; } } else { n = 1; } if (ngx_list_init(&amp;cycle-&gt;shared_memory, pool, n, sizeof(ngx_shm_zone_t)) != NGX_OK) { ngx_destroy_pool(pool); return NULL; } 这段代码的意思是遍历old_cycle，统计上一次系统中分配了多少块共享内存，接着就按这个数据初始化当前cycle中共享内存的规模。 2) 准备日志nginx启动可能出错，出错就要记录到错误日志中。而错误日志本身也是配置的一部分，不解析完配置，nginx就无法知道日志放哪。所以nginx通过使用上一个周期的错误日志来记录解析配置时发生的错误，而在配置解析完成以后，nginx就用新的错误日志替换旧的错误日志。具体代码摘抄如下，以说明nginx解析配置时使用old_cycle的错误日志： log = old_cycle-&gt;log; pool-&gt;log = log; cycle-&gt;log = log; 3）准备数据结构主要是两个数据结果，一个是ngx_cycle_t结构，一个是ngx_conf_t结构(放在nginx_cycle_t-&gt;conf_ctx。前者是一个周期的总结构体，保存着所有模块存储配置项的结构体的指针，它首先是一个数组，每个数组成员又是一个指针，这个指针指向另一个存储着指针的数组。具体代码如下：`c//准备所有核心模块存储配置项的结构体for (i = 0; ngx_modules[i]; i++) {//当ngx_modules[i]不为空的时候继续执行 if (ngx_modules[i]-&gt;type != NGX_CORE_MODULE) { continue; } module = ngx_modules[i]-&gt;ctx; if (module-&gt;create_conf) {//如果当前核心模块有create_conf函数 rv = module-&gt;create_conf(cycle);//返回的是模块定义的参数conf if (rv == NULL) { ngx_destroy_pool(pool); return NULL; } cycle-&gt;conf_ctx[ngx_modules[i]-&gt;index] = rv;//因为需要存储这些模块的conf } } //初始此cycle中的每个指令的上下文 //conf存储每个指令的属性（上下文） conf.ctx = cycle-&gt;conf_ctx;//这个conf_ctx明明存放的是conf为什么要传递给ctx? conf.cycle = cycle; conf.pool = pool; conf.log = log; conf.module_type = NGX_CORE_MODULE; conf.cmd_type = NGX_MAIN_CONF; 2. 真正开始解析 ```c if (ngx_conf_param(&amp;conf) != NGX_CONF_OK) { environ = senv; ngx_destroy_cycle_pools(&amp;conf); return NULL; } if (ngx_conf_parse(&amp;conf, &amp;cycle-&gt;conf_file) != NGX_CONF_OK) { environ = senv; ngx_destroy_cycle_pools(&amp;conf); return NULL; } 第一个if解析nginx命令行参数’-g’加入的配置。第二个if解析nginx配置文件。好的设计就体现在接口极度简化，模块之间的耦合非常低。这里只使用区区10行完成了配置的解析。 配置解析其实是解析并调用相应handler函数 配置解析模块在ngx_conf_file.c中实现。模块提供的接口函数主要是ngx_conf_parse，另外，模块提供一个单独的接口ngx_conf_param，用来解析命令行传递的配置，当然，这个接口也是对ngx_conf_parse的包装。ngx_conf_parse函数支持三种不同的解析环境： parse_file：解析配置文件； parse_block：解析块配置。块配置一定是由“{”和“}”包裹起来的； parse_param：解析命令行配置。命令行配置中不支持块指令。 配置解析一个递归的过程。nginx首先解析core模块的配置。core模块提供一些块指令，这些指令引入其他类型的模块，nginx遇到这些指令，就重新迭代解析过程，解析其他模块的配置。这些模块配置中又有一些块指令引入新的模块类型或者指令类型，nginx就会再次迭代，解析这些新的配置类型。比如，nginx遇到“events”指令，就重新调用ngx_conf_parse()解析event模块配置，解析完以后ngx_conf_parse()返回，nginx继续解析core模块指令，直到遇到“http”指令。nginx再次调用ngx_conf_parse()解析http模块配置的http级指令，当遇到“server”指令时，nginx又一次调用ngx_conf_parse()解析http模块配置的server级指令。了解了nginx解析配置的流程，我们来看其中的关键函数ngx_conf_parse()。 ngx_conf_parse()解析配置分成两个主要阶段，一个是词法分析，一个是指令解析。 词法分析通过ngx_conf_read_token()函数完成。指令解析有两种方式，1) 是使用nginx内建的指令解析机制，2) 是使用第三方自定义指令解析机制。自定义指令解析可以参见下面的代码： if (cf-&gt;handler) { rv = (*cf-&gt;handler)(cf, NULL, cf-&gt;handler_conf); if (rv == NGX_CONF_OK) { continue; } if (rv == NGX_CONF_ERROR) { goto failed; } ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, rv); goto failed; } nginx内建的指令解析机制分为四个步骤1) 只有处理的模块的类型是NGX_CONF_MODULE或者是当前正在处理的模块类型，即嵌套块中子模块必须与母模块类型相同，才可能被执行 ngx_modules[i]-&gt;type != NGX_CONF_MODULE &amp;&amp; ngx_modules[i]-&gt;type != cf-&gt;module_type 2) 匹配指令名，判断指令用法是否正确。 a. 指令的Context必须当前解析Context相符； !(cmd-&gt;type &amp; cf-&gt;cmd_type) b. 指令的Context必须当前解析Context相符； !(cmd-&gt;type &amp; cf-&gt;cmd_type) c. 非块指令必须以“;”结尾 (cmd-&gt;type &amp; NGX_CONF_BLOCK) &amp;&amp; last != NGX_CONF_BLOCK_START d. 指令参数个数必须正确。注意指令参数有最大值NGX_CONF_MAX_ARGS，目前值为8。 3) 取得指令工作的conf指针，就是模块定义的该模块感兴趣的配置参数结构体conf if (cmd-&gt;type &amp; NGX_DIRECT_CONF) { conf = ((void **) cf-&gt;ctx)[ngx_modules[i]-&gt;index]; } else if (cmd-&gt;type &amp; NGX_MAIN_CONF) { conf = &amp;(((void **) cf-&gt;ctx)[ngx_modules[i]-&gt;index]); } else if (cf-&gt;ctx) { confp = *(void **) ((char *) cf-&gt;ctx + cmd-&gt;conf); if (confp) { conf = confp[ngx_modules[i]-&gt;ctx_index]; } } 4) 执行指令解析回调函数rv = cmd-&gt;set(cf, cmd, conf);//cf是指令的属性结构体，conf是模块定义的配置参数结构体 http指令块解析ngx_http_module模块（核心模块）中定义了http指令结构，如下： { ngx_string(&quot;http&quot;), NGX_MAIN_CONF|NGX_CONF_BLOCK|NGX_CONF_NOARGS, ngx_http_block, 0, 0, NULL } 当遇到http指令时，会执行ngx_http_block函数 //从ngx_modules中载入所有的http模块，并且给他们按顺序排号（ctx_index，在http模块中的序号 ngx_modules[m]-&gt;ctx_index = ngx_http_max_module++; //调用所有http模块的create，并存入main_conf,srv_conf,loc_conf for (m = 0; ngx_modules[m]; m++) { if (ngx_modules[m]-&gt;type != NGX_HTTP_MODULE) { continue; } module = ngx_modules[m]-&gt;ctx;//里面的ctx一般指向http_module mi = ngx_modules[m]-&gt;ctx_index; if (module-&gt;create_main_conf) { ctx-&gt;main_conf[mi] = module-&gt;create_main_conf(cf); if (ctx-&gt;main_conf[mi] == NULL) { return NGX_CONF_ERROR; } } } //调用所有http模块的preconfiguration module-&gt;preconfiguration(cf) //解析http{} block cf-&gt;module_type = NGX_HTTP_MODULE;//设置好上下文的参数 cf-&gt;cmd_type = NGX_HTTP_MAIN_CONF; rv = ngx_conf_parse(cf, NULL); //遍历每个模块，执行init_main_conf函数，并把cmcf和每个模块合并 rv = module-&gt;init_main_conf(cf, ctx-&gt;main_conf[mi]); rv = ngx_http_merge_servers(cf, cmcf, module, mi);//module是指的ngx_http_module_t server块解析解析到server块时，ngx_conf_handler会调用ngx_http_core_module的ngx_http_core_server ngx_http_core_server(ngx_conf_t *cf, ngx_command_t *cmd, void *dummy){ //创建srv_ctx配置上下文 ctx = ngx_pcalloc(cf-&gt;pool, sizeof(ngx_http_conf_ctx_t)); //cf-&gt;ctx为http_ctx配置上下文 http_ctx = cf-&gt;ctx; //main_conf共用同一个（server块中不会有NGX_HTTP_MAIN_CONF类型的配置，所以其实是不需要main_conf的） ctx-&gt;main_conf = http_ctx-&gt;main_conf; ctx-&gt;srv_conf = ngx_pcalloc(cf-&gt;pool, sizeof(void *) * ngx_http_max_module); ctx-&gt;loc_conf = ngx_pcalloc(cf-&gt;pool, sizeof(void *) * ngx_http_max_module); //遍历所有http模块，调用其create_srv_conf方法和create_loc_conf创建相应配置结构 for (i = 0; ngx_modules[i]; i++) { if (ngx_modules[i]-&gt;type != NGX_HTTP_MODULE) { continue; } module = ngx_modules[i]-&gt;ctx; if (module-&gt;create_srv_conf) { mconf = module-&gt;create_srv_conf(cf); ctx-&gt;srv_conf[ngx_modules[i]-&gt;ctx_index] = mconf; } if (module-&gt;create_loc_conf) { mconf = module-&gt;create_loc_conf(cf); ctx-&gt;loc_conf[ngx_modules[i]-&gt;ctx_index] = mconf; } } //注意这里实现将srv_ctx上下文添加到http_ctx配置上下文； //ngx_http_core_module模块是第一个http模块。获取其在此函数创建的srv_conf类型的配置结构ngx_http_core_srv_conf_t；将其ctx字段指向此函数创建的ctx cscf = ctx-&gt;srv_conf[ngx_http_core_module.ctx_index]; cscf-&gt;ctx = ctx; //main_conf是http_ctx上下文的数组；获取其创建的main_conf类型的配置结构ngx_http_core_main_conf_t； //将此函数产生的srv_ctx配置的上下文结构ngx_http_core_srv_conf_t添加到http_ctx配置上下文的ngx_http_core_main_conf_t配置结构的servers数组 cmcf = ctx-&gt;main_conf[ngx_http_core_module.ctx_index]; cscfp = ngx_array_push(&amp;cmcf-&gt;servers);//返回数组尾部的地址 *cscfp = cscf; //修改cf的配置上下文，模块类型，指令类型；原始cf暂存在pcf变量 pcf = *cf; cf-&gt;ctx = ctx; cf-&gt;cmd_type = NGX_HTTP_SRV_CONF; //解析server块中的配置；注意此时配置上下文为srv_ctx rv = ngx_conf_parse(cf, NULL); //还原cf *cf = pcf; location块的解析解析到server块时，ngx_conf_handler会调用ngx_http_core_module的ngx_http_core_location static char * ngx_http_core_location(ngx_conf_t *cf, ngx_command_t *cmd, void *dummy){ //创建loc_conf上下文 ctx = ngx_pcalloc(cf-&gt;pool, sizeof(ngx_http_conf_ctx_t)); //cf-&gt;ctx指向srv_conf上下文 pctx = cf-&gt;ctx; //main_conf与srv_conf与srv_ctx上下文公用； //（location块中不会有NGX_HTTP_MAIN_CONF和NGX_HTTP_SRV_CONF类型的配置，所以其实是不需要main_conf和srv_conf的） ctx-&gt;main_conf = pctx-&gt;main_conf; ctx-&gt;srv_conf = pctx-&gt;srv_conf; ctx-&gt;loc_conf = ngx_pcalloc(cf-&gt;pool, sizeof(void *) * ngx_http_max_module); //遍历所有http模块，调用其create_loc_conf方法创建相应配置结构 for (i = 0; ngx_modules[i]; i++) { if (ngx_modules[i]-&gt;type != NGX_HTTP_MODULE) { continue; } module = ngx_modules[i]-&gt;ctx; if (module-&gt;create_loc_conf) { ctx-&gt;loc_conf[ngx_modules[i]-&gt;ctx_index] = module-&gt;create_loc_conf(cf); } } //使得我们在这个函数创建的ngx_http_core_loc_conf_t能指向这个函数创建的全部loc_conf clcf = ctx-&gt;loc_conf[ngx_http_core_module.ctx_index]; clcf-&gt;loc_conf = ctx-&gt;loc_conf; //获取http_ctx的ngx_http_core_loc_conf_t pclcf = pctx-&gt;loc_conf[ngx_http_core_module.ctx_index]; //将此函数创建的clcf（包含全部的loc_conf)放入http_ctx的locations中 //locations是一个双向链表，链表结构也挺有意思的，有兴趣的读者可以研究下 if (ngx_http_add_location(cf, &amp;pclcf-&gt;locations, clcf) != NGX_OK) { } } 想看的函数ngx_conf_parse","categories":[],"tags":[{"name":"后端","slug":"后端","permalink":"https://xixfeng.github.io/tags/后端/"}],"keywords":[]}]}